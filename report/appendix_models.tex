\clearpage
\makeatletter
\efloat@restorefloats
\makeatother


\begin{appendix}
\section{}
\hypertarget{statistical-models}{%
\subsection{Statistical models}\label{statistical-models}}

We are using the Bayesian framework (e.g. Farrell \& Lewandowsky, 2018;
Gelman et al., 2014; Lee \& Wagenmakers, 2014) to implement 5
statistical models of writing and to evaluate which -- the serial or the
cascading view -- captures more successfully the data that arise during
keyboard typing. In other words, we use statistical models to map
between the data and the theoretically assumed process that generates
the data to then compare the predictive power of those models. The
models presented in this section build on one another so that later
model include assumptions of earlier models.

\hypertarget{serial-model-of-writing}{%
\subsubsection{Serial model of writing}\label{serial-model-of-writing}}

\hypertarget{single-distribution-gaussian}{%
\paragraph{Single distribution
Gaussian}\label{single-distribution-gaussian}}

Under the serial view, all planning must be completed priors to the
production onset of the corresponding planning unit. The resulting IKI
is sometimes faster or slower depending on, among others,
psycholinguistic factors. For example, the interval before a word is
shorter for easily retrievable high-frequency words, or longer for low
frequency words, shorter for words with fewer graphemes, syllables, and
morphemes. There are word-specific factors that influence the IKI that
precedes a word but these are beyond the scope of our analysis. We will
capture the variability associated with word-features by assuming that
before-word IKIs can be described as coming from a distribution that is
normal (Gaussian) with two parameters, an unknown mean \(\mu\), that
describes the average IKI associated with word-level planning, and a
standard deviation \(\sigma_\text{e}^2\), that captures the variability
associated with factors that we did not further specify in the model.
This can be expressed as
\(\text{iki}_\text{before word} \sim \text{N}(\mu, \sigma_\text{e}^2)\).
Of importance is the estimated posterior value of \(\mu\) as this value
captures that time it takes to mentally plan a word.

We can extend this simple model of word-planning to other linguistic
location. We introduce earlier that larger linguistic edges are
associated with planning on higher levels. For example, at sentence
boundaries, planning needs to happen for word-level properties -- which
was captured above as average IKI \(\mu\) -- but also higher linguistic
planning such as clause-level meaning, and dependencies of the
sentence-initial noun (Nottbusch et al., 2007; Roeser et al., 2019).
Regression formulas can, and commonly are, used to decompose \(\mu\) and
capture that there is a change in the outcome variable associated with
another factors. We can decompose \(\mu\) as
\(\mu = \alpha + \beta \cdot \text{x}_\text{sentence[0,1]}\) so that
when \(\text{x}_\text{sentence}\) takes on the value 0, the equation
reduces to \(\mu = \alpha\) which is the average IKI for word
boundaries. However when \(\text{x}_\text{sentence}\) takes on the value
1, the average IKI for word boundaries \(\alpha\) is incremented by a
sentence-related slowdown of \(\beta\) msecs. Therefore the value of the
\(\beta\) parameter represents the additional cognitive demand
associated with sentence-initial planning. The application of such a
statistical model to the data will then provide us with an estimate of
the parameter value that can be used for statistical inference
(e.g.~whether there is a statistically meaningful difference between IKI
associated with words and sentences).

For computational ease, we implemented the differences associated with
transition locations as \(\beta_\text{location}\) in equation
\ref{eq:unimodgaus} where \(\text{location}\) is taking on an index for
each transition location. Therefore, the model will return one \(\beta\)
per transition location that capture the posterior distribution of
average IKIs. Also the decomposition of \(\mu\) allows us to address the
fact that some writers are faster than other writers by introducing a
parameter for what is typically called random intercepts
\(u_\text{participant}\). The random intercepts term
\(u_\text{participant}\) is constrained so that the value it takes on
comes from a normal distribution with a mean of 0 and a standard
deviation of \(\sigma_\text{p}^2\). This value is therefore the
difference between the overall posterior estimate and estimate average
IKI of a particular participant (i.e.~a positive value indicates that a
participant is slower than average, a negative value indicates that a
participant is faster than average).

The final model is shown in equation \ref{eq:unimodgaus} and represents
a Gaussian mixed effects model.

\begin{equation}
\begin{aligned}
(\#eq:unimodgaus)
\text{iki}_i \sim\text{ } & \text{N}(\mu_i, \sigma_\text{e}^2)\\
\text{where: } & \mu_i = \beta_\text{location[i]} + u_\text{participant[i]}\\
& u_\text{participant} \sim \text{N}(0, \sigma_\text{p}^2)\\
\text{constraint: } & \sigma_\text{e}^2, \sigma_\text{p}^2>0
\end{aligned}
\end{equation}

Note aside that standard-deviation parameters were constrained to be
positive because standard deviations can never be negative.

\hypertarget{single-distribution-log-gaussian}{%
\paragraph{Single distribution
log-Gaussian}\label{single-distribution-log-gaussian}}

The previous model assumes that the data-generating process is a
Gaussian distribution. The next model is largely identical to the
previous model but instead of assuming a Gaussian, we assume that the
data come from a log-normal (log-Gaussian) distribution. There are, at
least, two arguments for using a log-Gaussian distribution: (1)
log-Gaussians are zero-bound; in contrast to Gaussians, a log-Gaussian
does not allow negative values. IKIs, as the distance between two
subsequent key-down events, must be positive. The lower bound is
delimited by a person's ability to move their fingers and keyboard
polling. (2) the log-scale is known to be a better match for data from
human behaviour and motor responses. In particular, a Gaussian
distribution assumes that units are scaled linearly. For example, a
difference of 25 msecs is the same between 100 and 125 msecs as between
5 secs and 5,025 msecs. This does not necessarily map onto the
psychological interpretation for short and long keyintervals. For
example effects that result on the motor level within words (e.g.~typing
a high vs a low frequency bigram) are smaller than differences that are
due to high levels of processing (retrieving a word in an L1 or L2). In
other words, although an effect of 25 msecs is large in the context
overall fast keyintervals, it is small in the context of overall slow
intervals. Log-Gaussian distributions are a natural way of translating a
linear scale to an exponential scale so that a 25 msecs difference on
the lower end of the IKI scale (motor activity) is more meaningful than
a 25 msecs difference on the upper end of the IKI scale (retrieving
words, planning sentences).

The model can be described as in equation \ref{eq:unimodloggaus} in
which the distribution \(\text{N}()\) was replaced by \(\text{logN}()\).

\begin{equation}
\begin{aligned}
(\#eq:unimodloggaus)
\text{iki}_i \sim\text{ } & \text{logN}(\mu_i, \sigma_\text{e}^2) \\
\text{where: } &
\mu_i = \beta_\text{location[i]} + u_\text{participant[i]}\\
& u_\text{participant} \sim \text{N}(0, \sigma_\text{p}^2) \\
\text{constraint: } & \sigma_\text{e}^2, \sigma_\text{p}^2>0
\end{aligned}
\end{equation}

\hypertarget{single-distribution-unequal-variance-log-gaussian}{%
\paragraph{Single distribution unequal-variance
log-Gaussian}\label{single-distribution-unequal-variance-log-gaussian}}

The third model representing the serial view of writing is a single
distribution unequal-variance model that, except for the
unequal-variance assumption is identical to the model presented in the
previous section. The previous models modelled IKIs as a function of
transition location so that the estimated average IKI depends on the
position of an IKI in the text. The variance associated with the
estimated IKIs for each transition location was assumed to be identical
(equal variance). This assumption is however not in line with what we
know about data from human behaviour. Longer latencies are known to be
associated with a larger variances for both response-time data in
particular (Wagenmakers \& Brown, 2007) and human motor behaviour in
general (SchÃ¶ner, 2002; Wing \& Kristofferson, 1973). For IKIs pauses at
larger linguistic edges are plausibly associated with a larger variance
because of the larger number of associated processes. Therefore, in
equation \ref{eq:unimoduv}, we introduce the assumption that standard
deviation \(\sigma_{e_\text{location}}^2)\) varies by transition
location.

\begin{equation}
\begin{aligned}
(\#eq:unimoduv)
\text{iki}_i \sim\text{ } & \text{logN}(\mu_i, \sigma_{e_\text{location[i]}}^2) \\
\text{where: } & \mu_i = \beta_\text{location[i]} + u_\text{participant[i]}\\
 & u_\text{participant} \sim \text{N}(0, \sigma_\text{p}^2) \\
 \text{constraint: } & \sigma_\text{e}^2, \sigma_\text{p}^2>0
\end{aligned}
\end{equation}

\hypertarget{parallel-cascading-model-of-writing}{%
\subsubsection{Parallel cascading model of
writing}\label{parallel-cascading-model-of-writing}}

The following two models are are extensions of the models introduced for
the serial view. Crucially, the cascading view allows planning to happen
in parallel to production. Therefore, we will reduce the constrain of
the serial models that requires all planning to be completed before
writing onset. This is done by assuming that IKIs come from a weighted
combination of two distributions.

\hypertarget{two-distributions-log-gaussian-mixture-model-constrained}{%
\paragraph{Two-distributions log-Gaussian mixture model
(constrained)}\label{two-distributions-log-gaussian-mixture-model-constrained}}

This model extends the assumption of the previous model that processing
that involves higher levels of activation lead to longer pauses. Instead
of assuming that there is one process that is shifted for IKIs of larger
linguistic edges, we introduce the assumption that pauses at larger
linguistic edges are more likely but not obligatory. This is achieved by
modelling IKIs as coming from a weighted mixture of two distributions
associated with two different states:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Activation can flow into keystrokes without interruption. These fluent
  keystroke transitions are merely constrained by a person's ability to
  move their finger and will be captured by the \(\beta\) parameter in
  equation \ref{eq:bimodcon}. Note that \(\beta\) is represented in both
  log-Gaussian distributions in equation \ref{eq:bimodcon} referring to
  the same unknown parameter.
\item
  Interruptions in the activation flow from higher to lower levels
  result in longer keystroke intervals, when words or their spelling
  could not be retrieved in time. The slowdown for these hesitant
  transitions is captured by \(\delta\) in the first line of equation
  \ref{eq:bimodcon}. The magnitude of the slowdown is associated with
  transition location. This is because delays at larger linguistic units
  are likely to be associated with higher level planning. By
  constraining \(\delta\) to be positive, it captures how much longer
  hesitant IKIs in addition to \(\beta\).
\end{enumerate}

\begin{equation}
\begin{aligned}
(\#eq:bimodcon)
\text{iki}_{i} \sim\text{ } & \theta_\text{location[i], participant[i]} \cdot \text{LogN}(\beta + \delta_\text{location[i]} + u_\text{participant[i]}, \sigma_{e'_\text{location[i]}}^2) + \\
  & (1 - \theta_\text{location[i], participant[i]}) \cdot \text{LogN}(\beta + u_\text{participant[i]}, \sigma_{e_\text{location[i]}}^2)\\
\text{where: } & u_\text{participant} \sim \text{N}(0, \sigma_\text{p}^2) \\
\text{constraint: } & \delta, \sigma_{e}^2, \sigma_\text{e'}^2, \sigma_\text{p}^2>0\\
        & \sigma_{e'}^2 > \sigma_{e}^2\\
        & 0 < \theta < 1
\end{aligned}
\end{equation}

The first line of equation \ref{eq:bimodcon} represents the distribution
of hesitant key transitions, and the second line represents fluent key
transitions. Each of these two distributions is associated with the
mixing weight \(\theta\) which is a proportion that is constrained to be
larger than 0 and smaller than 1. \(\theta\) is here parameterised to
represent the probability that an IKI is associated with the
distribution of long IKIs. This probability is inversely related to the
mixing weight of the distribution of short IKIs by \(1-\theta\) . In
other words, a larger weight of one distribution inevitably means a
lower weight for the other distribution. The weights of both
distributions must sum to 1. We will call this parameter the probability
of hesitant transitions.

The probability of hesitant transitions is assumed to vary as a function
of both transition location and participants. In line with the
literature discussed in the introduction, we assume that pauses are more
likely at larger linguistic edges. As pausing is subject to individual
differences and writing style (skills), we also assume that some
participants pause more at certain transition locations and other
participants pause less. This is akin to what is known as a random
by-participant slopes model.

Lastly, we carried over the unequal variance assumption and let the
standard deviations \(\sigma_{e'}^2\) and \(\sigma_{e}^2\) vary by
transition location. In addition we constrained the variances so that
\(\sigma_{e'}\) associated with the distribution of typing disfluencies
is larger than the variance associated with fluent transitions
\(\sigma_e\) (see Vasishth, Chopin, et al., 2017; Vasishth, JÃ¤ger, et
al., 2017). This was achieved by introducing a parameter
\(\sigma_\text{diff}\). The consequence is that fluent transitions are
assumed to come from a narrower distribution than hesitant transitions.

\hypertarget{two-distributions-log-gaussian-mixture-model-unconstrained}{%
\paragraph{Two-distributions log-Gaussian mixture model
(unconstrained)}\label{two-distributions-log-gaussian-mixture-model-unconstrained}}

The size of a fluent key-transition does not necessarily vary by
transition location. In other words, the parameter \(\beta\) is the same
for before-sentence, before-word, and within-word transitions. This is
what we called a constrained model. However, letter bigrams and trigrams
may be systematically executed faster than transitions between between
space and a letter (REFERENCE?) or complex keystrokes that comprise
space and shift-letter combinations for upper case characters before
sentences (we will address the latter possibility in the results
section). This is because bigrams / trigrams might be stored, retrieved
and executed as motor codes but not transitions to a space following a
word or between a space or shift key press preceding a letter. Also,
because of the necessarily larger number of within-word transitions, as
opposed to before-word and sentence transitions, the posterior of the
constrained model is dominated by within-word transition data. We
therefore also implemented an unconstrained model that assumes that the
size of fluent transitions varies across transition locations.

In this model we assume that \(\beta\) varies by transition-location as
illustrated in equation \ref{eq:bimoduncon}.

\begin{equation}
\begin{aligned}
(\#eq:bimoduncon)
\text{iki}_{i} \sim\text{ } & \theta_\text{location[i], participant[i]} \cdot \text{LogN}(\beta_\text{location[i]} + \delta_\text{location[i]} + u_\text{participant[i]}, \sigma_{e'_\text{location[i]}}^2) + \\
  & (1 - \theta_\text{location[i], participant[i]}) \cdot \text{LogN}(\beta_\text{location[i]} + u_\text{participant[i]}, \sigma_{e_\text{location[i]}}^2)\\
    \text{where: }  & u_\text{participant} \sim \text{N}(0, \sigma_\text{p}^2) \\
\text{constraint: } & \delta, \sigma_{e}^2, \sigma_\text{e'}^2, \sigma_\text{p}^2>0\\
        & \sigma_{e'}^2 > \sigma_{e}^2\\
        & 0 < \theta < 1
\end{aligned}
\end{equation}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-farrell2018computational}{}%
Farrell, S., \& Lewandowsky, S. (2018). \emph{Computational modeling of
cognition and behavior}. Cambridge University Press.

\leavevmode\hypertarget{ref-gelman2014}{}%
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., \&
Rubin, D. B. (2014). \emph{Bayesian data analysis} (3rd ed.). Chapman;
Hall/CRC.

\leavevmode\hypertarget{ref-lee2014bayesian}{}%
Lee, M. D., \& Wagenmakers, E.-J. (2014). \emph{Bayesian cognitive
modeling: A practical course}. Cambridge University Press.

\leavevmode\hypertarget{ref-not07}{}%
Nottbusch, G., Weingarten, R., \& Sahel, S. (2007). From written word to
written sentence production. In M. Torrance, L. van Waes, \& D. W.
Galbraith (Eds.), \emph{Writing and cognition: Research and
applications} (Vol. 20, pp. 31--53). Elsevier.

\leavevmode\hypertarget{ref-roeser2018advance}{}%
Roeser, J., Torrance, M., \& Baguley, T. (2019). Advance planning in
written and spoken sentence production. \emph{Journal of Experimental
Psychology: Learning, Memory, and Cognition}, \emph{45}(11), 1983--2009.
\url{https://doi.org/10.1037/xlm0000685}

\leavevmode\hypertarget{ref-schoner2002timing}{}%
SchÃ¶ner, G. (2002). Timing, clocks, and dynamical systems. \emph{Brain
and Cognition}, \emph{48}(1), 31--51.

\leavevmode\hypertarget{ref-vasishth2017}{}%
Vasishth, S., Chopin, N., Ryder, R., \& Nicenboim, B. (2017). Modelling
dependency completion in sentence comprehension as a {B}ayesian
hierarchical mixture process: {A} case study involving {C}hinese
relative clauses. \emph{ArXiv e-Prints}.

\leavevmode\hypertarget{ref-vasishth2017feature}{}%
Vasishth, S., JÃ¤ger, L. A., \& Nicenboim, B. (2017). Feature overwriting
as a finite mixture process: Evidence from comprehension data.
\emph{arXiv Preprint arXiv:1703.04081}.

\leavevmode\hypertarget{ref-wagenmakers2007linear}{}%
Wagenmakers, E.-J., \& Brown, S. (2007). On the linear relation between
the mean and the standard deviation of a response time distribution.
\emph{Psychological Review}, \emph{114}(3), 830--841.
\url{https://doi.org/10.1037/0033-295X.114.3.830}

\leavevmode\hypertarget{ref-wing1973response}{}%
Wing, A. M., \& Kristofferson, A. B. (1973). Response delays and the
timing of discrete motor responses. \emph{Perception \& Psychophysics},
\emph{14}(1), 5--12.

\end{CSLReferences}
\end{appendix}
