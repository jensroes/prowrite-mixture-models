# Statistical models

We used the Bayesian framework [e.g. @farrell2018computational; @gelman2014; @lee2014bayesian] to implement four statistical models to evaluate which -- the serial or the parallel view -- captures interkey data more successfully. In other words, we use statistical models to map between the keystroke intervals and the theoretically assumed process that generates theses data to then compare the predictive power of those models. These models presented in this section are building on one another so that later models include assumptions of earlier models.

## Single-distribution Gaussian

Under the serial view, all planning must be completed prior to typing onset of the corresponding planning unit. The resulting interkey interval is sometimes faster or slower depending on, among others, psycholinguistic factors; i.e. the interval before a word is shorter for easily retrievable high-frequency words, or longer for low frequency words, shorter for words with fewer graphemes, syllables, and morphemes. For example, we can capture the variability associated with word-features by assuming that before-word interkey intervals can be described as coming from a distribution that is normal (Gaussian) with two parameters, an unknown mean $\mu$, that describes the average interkey interval associated with word-level planning, and a standard deviation $\sigma_\text{e}^2$, that captures the remaining variability associated with factors that we did not further specify in the model. This can be expressed as $\text{iki}_\text{before word} \sim \text{log}\mathcal{N}(\mu, \sigma_\text{e}^2)$ where $\text{iki}_\text{before word}$ are all interkey intervals immediately proceeding a word. Of importance is the estimated posterior value of $\mu$ as this value captures average time needed to mentally prepare a word.

We can extend this simple model of word-planning to other linguistic location. Larger linguistic edges are typically understood as being associated with planning on higher levels. For example, at sentence boundaries, planning needs to happen for word-level properties -- which we captured before as average interkey interval $\mu$ -- but also higher linguistic planning such as clause-level meaning, and dependencies of the sentence-initial noun [@roeser2018advance; @not07]. Using regression formulae we can capture changes in the outcome variable that are associated with other factors. More specifically we can decompose $\mu$ as $\mu = \alpha + \beta \cdot \text{x}_\text{sentence[0,1]}$ so that when $\text{x}_\text{sentence}$ takes on the value 0, the equation reduces to $\mu = \alpha$ which is then the average interkey interval for word boundaries but, when $\text{x}_\text{sentence}$ takes on the value 1, the average interkey interval for word boundaries $\alpha$ is incremented by a slowdown associated with sentence boundaries of $\beta$ msecs. Therefore the value of the $\beta$ parameter represents the additional cognitive demand associated with sentence-initial planning. The application of such a statistical model to the data will therefore provide us with an estimate of the parameter value that can be used for statistical inference (e.g. whether there is a statistically meaningful difference associated with word and sentence boundaries).

We implemented the average interkey interval associated with a given text locations as $\beta_\text{location}$ in equation \ref{eq:unimodgaus} where $\text{location}$ is taking on an index for each transition location. Therefore, the model returns three distributions of $\beta$ values that capture the posterior distribution of average interkey intervals for each transition location. The decomposition of $\mu$ allows us to address the fact that some writers are faster than other writers by introducing a parameter for what is called random intercepts $u_\text{participant}$. The random intercepts term $u_\text{participant}$ is constrained so that the value it takes on comes from a normal distribution with a mean of 0 and a standard deviation of $\sigma_\text{p}^2$. The $u_\text{participant}$ value is therefore the difference between the overall posterior estimate and estimate average interkey interval of a particular participant (i.e. a positive value indicates that a participant is slower than average; a negative value indicates that a participant is faster than average).

The resulting model is shown in equation \ref{eq:unimodgaus} and represents a Gaussian mixed-effects model  


\begin{equation}
\begin{aligned}
(\#eq:unimodgaus)
\text{iki}_i \sim\text{ } & \text{log}\mathcal{N}(\mu_i, \sigma_\text{e}^2)\\
\text{where: } & \mu_i = \beta_\text{location[i]} + u_\text{participant[i]}\\
& u_\text{participant[i]} \sim \text{N}(0, \sigma_\text{p}^2)\\
\text{constraint: } & \sigma_\text{e}^2, \sigma_\text{p}^2>0
\end{aligned}
\end{equation}

where $i$ iterates over every interkey interval $i \in 1 \dots N$ with $N$ being the total number of interkey intervals; thus $\text{location[i]}$ is the transition location associated with the $i$^\text{th} keystroke and $\text{participant[i]}$ is the participant associated with the $i$^\text{th} keystroke.

Standard-deviation parameters were constrained to be positive because standard deviations can never be negative. 

## Single distribution log-Gaussian

The previous model assumes that the data-generating process follows a Gaussian distribution. As a consequence the Gaussian model generates values that are symmetrically distributed and, in principle, allows values to be negative. The model presented in this section is largely identical to the previous model but instead of assuming a Gaussian, we assume that the data come from a log-normal (log-Gaussian) distribution. 

There are, at least, two arguments for using a log-Gaussian distribution: (1) log-Gaussians are zero-bound; in contrast to Gaussians, a log-Gaussian does not allow negative values. This is a desirable property because the distance between two subsequent key events must be positive. Instead the lower bound is delimited by a person's ability to move their fingers and keyboard polling. (2) the log-scale is known to be a better match for data from human behaviour and motor responses. In particular, a Gaussian distribution assumes that units are scaled linearly. For example, a difference of 25 msecs is the same between 100 and 125 msecs as between 5 secs and 5,025 msecs. This does not map onto the psychological interpretation for short and long interkey intervals. For example effects that result from the motor level within words (e.g. typing a high vs a low frequency bigram) are smaller than differences that are due to high levels of processing (retrieving a word in an L1 or L2). In other words, an effect of 25 msecs can be large in the context of fast inerkey intervals but it is small in the context of overall slow intervals. Log-Gaussian distributions are a natural way of translating a linear scale to an exponential scale so that a 25 msecs difference on the lower end of interkey intervals (motor activity) is more psychologically meaningful than a 25 msecs difference on the upper end (retrieving words, planning sentences).

This model can be described as in equation \ref{eq:unimodloggaus} in which the distribution $\text{log}\mathcal{N}()$ was replaced by $\text{log}\mathcal{N}()$.

\begin{equation}
\begin{aligned}
(\#eq:unimodloggaus)
\text{iki}_i \sim\text{ } & \text{log}\mathcal{N}(\mu_i, \sigma_\text{e}^2) \\
\text{where: } &
\mu_i = \beta_\text{location[i]} + u_\text{participant[i]}\\
& u_\text{participant[i]} \sim \mathcal{N}(0, \sigma_\text{p}^2) \\
\text{constraint: } & \sigma_\text{e}^2, \sigma_\text{p}^2>0
\end{aligned}
\end{equation}

## Single distribution unequal-variance log-Gaussian

The third model is identical to the model presented in the previous section but removed the equal-variance assumption for the width of the distributions assocated with transition locations. The previous models modelled interkey intervals as a function of transition location meaning that the average interkey interval depends on its position in the text. The variance associated with the estimated interkey intervals for each transition location was assumed to be identical (equal variance). This assumption is however not in line with what we know about data from human motor behaviour. Longer latencies are known to be associated with a larger variances for both response-time data in particular [@wagenmakers2007linear] and human motor behaviour in general [@wing1973response;@schoner2002timing]. For interkey intervals pauses at larger linguistic edges can therefore be assumed to be associated with a larger variance also because of the larger number of associated processes. Therefore, in equation \ref{eq:unimoduv}, we introduced the assumption that standard deviations $\sigma_{e_\text{location}}^2)$ vary by transition location. 

\begin{equation}
\begin{aligned}
(\#eq:unimoduv)
\text{iki}_i \sim\text{ } & \text{log}\mathcal{N}(\mu_i, \sigma_{e_\text{location[i]}}^2) \\
\text{where: } & \mu_i = \beta_\text{location[i]} + u_\text{participant[i]}\\
 & u_\text{participant[i]} \sim \mathcal{N}(0, \sigma_\text{p}^2) \\
 \text{constraint: } & \sigma_\text{e}^2, \sigma_\text{p}^2>0
\end{aligned}
\end{equation}


## Two-distributions log-Gaussian mixture model 

The following model is an extension of the last single-distribution model. In contrast to the models introduced for the serial view, the cascading view allows planning to unfold in parallel to production. Therefore, we removed for the model presented in this section the constraint that all planning must be completed before writing onset. Instead of assuming that larger linguistic edges shift the distribution of interkey intervals as in the previous models, we modelled pauses in the distribution of interkey intervals, under the parallel view, to be more likely to occur at larger linguistic edges. This was achieved by introducing the model assumption that interkey intervals come from a weighted combination (mixure) of two distributions associated with two different states: 

1. Activation can flow into keystrokes without interruption. These fluent keystroke transitions are merely constrained by a person's ability to move their finger and were captured by $\beta$ in equation \ref{eq:bimodcon}. Note that $\beta$ is represented in both log-Gaussian distributions in equation \ref{eq:bimodcon} referring to the same unknown parameter. 

2. Interruptions in the activation flow from higher to lower levels result in longer keystroke intervals, when information, for example words or their spelling, could not be retrieved in time. The slowdown for these hesitant transitions is captured by $\delta$ in the first line of equation \ref{eq:bimodcon} which represents the magnitude of the slowdown associated with transition location. The slowdown $\delta$ was allowed to vary by transition locations because hesitations at larger linguistic units are likely to be associated with higher level planning which causes longer delays in the output. The $\delta$ parameter was constrained to be positive, so that it captures how much longer hesitant interkey intervals are in addition to $\beta$. 


\begin{equation}
\begin{aligned}
(\#eq:bimodcon)
\text{iki}_{i} \sim\text{ } & \theta_\text{location[i], participant[i]} \times \text{log}\mathcal{N}(\beta + \delta_\text{location[i]} + u_\text{participant[i]}, \sigma_{e'_\text{location[i]}}^2) + \\
  & (1 - \theta_\text{location[i], participant[i]}) \times \text{log}\mathcal{N}(\beta + u_\text{participant[i]}, \sigma_{e_\text{location[i]}}^2)\\
\text{where: } & u_\text{participant[i]} \sim \mathcal{N}(0, \sigma_\text{p}^2) \\
\text{constraint: } & \delta, \sigma_{e}^2, \sigma_\text{e'}^2, \sigma_\text{p}^2>0\\
		& \sigma_{e'}^2 > \sigma_{e}^2\\
		& 0 < \theta < 1
\end{aligned}
\end{equation}


The first line of equation \ref{eq:bimodcon} represents the distribution of hesitant interkey intervals; the second line represents fluent interkey intervals. Each of these two distributions is associated with the mixing weight $\theta$ which is a proportion that is constrained to be larger than 0 and smaller than 1. $\theta$ is here parameterised to represent the probability that an interkey interval is associated with the distribution of long interkey intervals, hence representing the hesitation probability. This probability is inversely related to the mixing weight of the distribution of short interkey intervals by $1-\theta$ because the weights of both distributions must sum to 1. In other words, a larger weight for one distribution inevitably means a lower weight for the other distribution. 

The probability of hesitant transitions is assumed to vary as a function of both transition location and participants. In line with the literature discussed in the introduction, we assumed that pauses are likely to vary across linguistic edges. As pausing is subject to individual differences and writing style (skills), we also assumed that some participants pause more at certain transition locations and other participants pause less. 

<!-- This is akin to what is known as a random by-participant slopes model.  -->

Lastly, we carried over the unequal variance assumption and let the standard deviations $\sigma_{e'}^2$ and $\sigma_{e}^2$ vary by transition location. In addition we constrained the variances so that $\sigma_{e'}$ associated with the distribution of typing hesitations is larger than the variance associated with fluent transitions $\sigma_e$ [see @vasishth2017; @vasishth2017feature].



<!-- ### Two-distributions log-Gaussian mixture model (unconstrained) -->

<!-- The size of a fluent key-transition does not necessarily vary by transition location. In other words, the parameter $\beta$ is the same for before-sentence, before-word, and within-word transitions. This is what we called a constrained model. However, letter bigrams and trigrams may be systematically executed faster than transitions between between space and a letter or complex keystrokes that comprise space and shift-letter combinations for upper case characters before sentences (we will address the latter possibility in the results section). This is because bigrams / trigrams might be stored, retrieved and executed as motor codes but not transitions to a space following a word or between a space or shift key press preceding a letter. Also, because of the necessarily larger number of within-word transitions, as opposed to before-word and sentence transitions, the posterior of the constrained model is dominated by within-word transition data. We therefore also implemented an unconstrained model that assumes that the size of fluent transitions varies across transition locations. -->

<!-- In this model we assume that $\beta$ varies by transition-location as illustrated in equation \ref{eq:bimoduncon}. -->

<!-- \begin{equation} -->
<!-- \begin{aligned} -->
<!-- (\#eq:bimoduncon) -->
<!-- \text{iki}_{i} \sim\text{ } & \theta_\text{location[i], participant[i]} \cdot \text{log}\mathcal{N}(\beta_\text{location[i]} + \delta_\text{location[i]} + u_\text{participant[i]}, \sigma_{e'_\text{location[i]}}^2) + \\ -->
<!--   & (1 - \theta_\text{location[i], participant[i]}) \cdot \text{log}\mathcal{N}(\beta_\text{location[i]} + u_\text{participant[i]}, \sigma_{e_\text{location[i]}}^2)\\ -->
<!-- 	\text{where: }  & u_\text{participant[i]} \sim \mathcal{N}(0, \sigma_\text{p}^2) \\ -->
<!-- \text{constraint: } & \delta, \sigma_{e}^2, \sigma_\text{e'}^2, \sigma_\text{p}^2>0\\ -->
<!-- 		& \sigma_{e'}^2 > \sigma_{e}^2\\ -->
<!-- 		& 0 < \theta < 1 -->
<!-- \end{aligned} -->
<!-- \end{equation} -->


