---
title             : "Ideas cascading into keystrokes -- modelling writing hesitations as finite mixture process"
shorttitle        : "Modelling writing hesitations"

csl               : "apa.csl" 

author: 
  - name          : "Jens Roeser"
    affiliation   : "1"
    address       : "50 Shakespeare St, Nottingham NG1 4FQ"
    corresponding : yes 
    email         : "jens.roeser@ntu.ac.uk"
  - name          : "Mark Torrance"
    affiliation   : "1"
  - name          : "Rianne Conijn"
    affiliation   : "2"
  - name          : "Evgeny Chukharev"
    affiliation   : "3"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Nottingham Trent University, United Kingdom"

  - id            : "2"
    institution   : "Artificial Intelligence Systems Institute, Eindhoven University of Technology, The Netherlands"

  - id            : "3"
    institution   : "Department of English, Iowa State University, Iowa"


abstract: Classical serial models view the process of producing a text as a descrete chain of pauses and writing bursts. In contrast, parallel cascading models of writing assume that planning is not complete at production onset and operates in parallel to writing execution. We implemented these two view in Bayesian statistical models and applied our models to key-stroke logs of 6 data sets from free text production. We reanalysed keystroke intervals at before-sentence transitions, before word transitions and within word transitions. Model comparisons demonstrated strong evidence in favour of the statistical implementation of the cascading view of the serial models across all data sets. Further we found that although pause durations are consistently longer for larger linguistic edges, the pause frequencies are not, but largely identical for before sentence and before word transition locations. Our results cannot be explained by the serial but are in line with the cascading view of writing.   


keywords: "Keystroke modelling; finite mixture models; Bayesian models; text composition"


bibliography      : ["references.bib"]


documentclass     : "apa7"
classoption       : "man"
output            : 
  papaja::apa6_pdf:
    keep_tex: TRUE
    includes:
      after_body: 
        - "appendix_models.tex"
        - "appendix_fit2data.tex"
#        - "appendix_postparam.tex"
        - "appendix_postparam_constr.tex"
        - "appendix_shift_constr.tex"
#        - "appendix_l2.tex"
        - "appendix_l2_constr.tex"
#        - "appendix_masking.tex"
        - "appendix_masking_constr.tex"
#        - "appendix_prepost.tex"
        - "appendix_prepost_constr.tex"
#        - "appendix_genre.tex"
        - "appendix_genre_constr.tex"
        - "appendix_transloc_constr.tex"
        - "appendix_transloc.tex"
        - "appendix_shift.tex"
        - "appendix_sim.tex"

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no
csquotes          : true

header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{graphicx}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{threeparttable}
  - \usepackage[normalem]{ulem}
  - \usepackage[utf8]{inputenc}
  - \usepackage{icomma}
  - \usepackage{pdflscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \DeclareCaptionFormat{cont}{#1 (cont.)#2#3\par}
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE, 
                      comment = NA,
                      cache.extra = R.version,
                      dev = "cairo_pdf",
                      fig.align = 'center', 
                      fig.width = 8, 
                      fig.height = 4.5,
                      width=90)

options(kableExtra.auto_format = FALSE)
dev.args <- list(pdf = list(type = "cairo"))
```

```{r libs, include=FALSE}
library(tidyverse)
library(janitor)
library(brms)
library(papaja)
library(polspline)
library(patchwork)
library(gridExtra)
library(grid)
library(gtable)
library(scales)
library(ggthemes)
source("../scripts/functions.R")
source("scripts/get_pdens_plot.R")

theme_set(theme_few(base_size = 12) +
            theme(strip.background = element_blank(),
                  legend.position = "top",
                  legend.justification = "right",
                  panel.grid = element_blank(),
                  panel.background = element_rect(fill = "transparent"), # bg of the panel
                  plot.background = element_rect(fill = "transparent", color = NA)))

options(scipen = 999)

beta_label <- "Fluent transition\nduration (in msecs)"
delta_label <- "Slowdown for hesitant\ntransitions (in secs)"
theta_label <- "Probability of hesitant\ntransitions"
```




Translating ideas into language involves a cascade of processes starting with a communicative intention, over the generation of an message, deciding with which part of the message to start the utterance, retrieving appropriate lexical materials and ordering them in an appropriate syntactic order before retrieving orthographic and / or phonological codes that are finally submitted to the motor programs that allow us to articulate language in speech, keyboard tying, handwriting, and signing [@bock2014syntactically; @wheeldon_konopka_2023; @van1991handwriting; @olive2014toward].

Writing is arguably more demanding than spoken language, because, among other reasons, writing requires spelling and we typically do not move our fingers or our pen as quickly as we articulate language sounds in speech. Consequently in writing more than in speech, we have to mentally buffer our linguistic plan before we can express it in language. Written communication is also less time constrained: speaking underlies fluency requirements and therefore has stronger pressure to plan information ahead of speaking [@roeser2018advance; @torrance2012written]. This is because in spoken production there is a pressure for fluency whereas hesitations in written output have no communicational implications for the reader [e.g. @Clark2002]. Indeed writers can pause when -- even in the middle of a word -- and for how long they want (within reason) without compromising communication. Written text production -- for example argumentative texts -- is therefore a combination for relatively fluent production bursts followed by pauses [@chenoweth2001fluency]. 

The study reported in this paper presents a comparison of two theories of how cognitive planning processes are coordinated in writing using implementations in statistical models. On the basis of those statistical models applied to 6 datasets, we revisit the often reported finding that edges of larger linguistic units are associated with longer pauses [@conijn2019understanding; @matsuhashi1981pausing; @flower1981pregnant; @wen06]. For example, sentence boundaries are generally thought of as involving longer pauses, compared to word boundaries, because participants engage in higher-level planning [@baaijen2012keystroke; @roeser2018advance; @medimorec2017disfluency; @medimorec2017pauses]. The same is generally observed for spoken utterances [@lee13; @meyer1996; @wheeldon2013] with some exceptions [@gri01]. We first discuss pause patterns in text production and introduce two possible cognitive frameworks in which these can be interpreted. Pause-patterns at linguistic edges are important, as they are symptomatic for the two cognitive theories of writing we discuss in the following.

<!-- a statistical framework for the analysis of keystroke data that captures not just the multilevel-structure of the data but also presents a principled way to capture the probabilistic nature of pauses in text composition. Hesitations are probabilistic in the sense that we cannot be predicted their occurrence deterministically. In principle we can produce several words and even sentences without pausing. However, hesitations are more of less likely to occur in certain contexts, for example, at sentence boundaries.  -->

Written text production is the result of a cascade of processes that starts with an communicative idea on the top forming a semantic representation, which is then fed into a grammatical encoder that is responsible for the retrieval of lexical names and, depending on theoretical viewpoint [@bock2014syntactically; @wheeldon_konopka_2023], the generation of syntactic representations, and finally flow from orthographic retrieval into motor plans for executing finger movements [@van1991handwriting; @olive2014toward]. The intervals between adjacent keystrokes during typing vary as a function of various factors. Explaining the factors that influence the duration of inter-keystroke intervals (IKI) requires a theory of how the various processes that transform intent into keypresses are structured. Processing may be serial. This means that the processing cascade operates on one unit at a time. Processing of the next unit can only start when the previous unit was fully processed from communicative intention to completion of the written output. Therefore longer IKIs represent periods when the writer is planning what to write next. The subsequent burst of fluent output is then the result this planning [@hayes2012evidence; @matsuhashi1981pausing; @alves2015progress; @schilperoord2002cognitive; @kaufer1986composing]. This is particularly reflected in the model proposed by @flower1980ynamics that characterises the writing process as a sequence of generating ideas, planning the text, translating the ideas into language, executing hand-writing or typing, and revising the texts [also @chenoweth2001fluency; @chenoweth2003inner].


This serial account is consistent with the finding that average IKI duration immediately before sentence-initial keystrokes are longer than before words and these are, in turn, longer than between mid-word key presses [@torrance2016adolescent; @wen02; @mohsen2021second]. However this account is not consistent with what we know about writing. Consider these two observations: 

First, utterances are not fully formed at production onset. Instead, syntax and lexical content and even details of the message itself are planned as the emergent result of in real-time process. This is well known for spoken production [e.g. @bock2014syntactically]. For writing, there is evidence that the time to keystroke / pen onset increases when a sentence starts with a more complex sentence-initial phrases [@not10; dam09; @roeser2018advance]. For example, in @roeser2018advance participants described moving arrays of images in simple utterances such as *A and the B moved above the C* that either started with a conjoined noun phrase (i.e. *A and the B moved*) or a simple noun (i.e., *A moved above the B and the C*). Importantly while the overall complexity of the utterance (in terms of length, number of phrases and noun) was held constant, sentences that started with a conjoined noun phrase increased the time to typing onset. This is not what one would expect if writers plan sentences (or even phrases) in full before production onset. In fact, eye movement data reported in @torrance2012written and @roeser2018advance demonstrate that writers do not plan the lexical form of the utterance beyond the first noun before production onset [similar to speech @gri01; @griffin2003reversed], although there was evidence that lexical pre-planning of the second noun (i.e. *the B*) was more likely for conjoined noun phrases.


Second, even though average sentence-initial IKIs tends to be longer, they still tend to be very rapid. For example, @medimorec2017pauses found that, in undergraduate students writing on familiar topics 71% of sentence-initial IKIs were less than 1 sec in duration. Also @ronneberg2022process reported that sentence-initial hesitations are rare, with over 50% of sentences preceded by very short pauses (around 430 msecs) and a mean of 1.2 seconds for the remainder of pauses. For comparison, this is less than mean written naming response latency for single objects [@torrance2018timed] and short sentences when describing arrays of images [@roeser2018advance] in a similar population. Despite the fact that writers can in principle pause when they want to, written composition is often remarkably fluent. The finding that sentence-initial IKIs are longer before words, maps onto a theory that assumes that planning a sentence requires the writer to pause and think before pressing the sentence-initial key. 


If utterances are not planned in full prior to writing onset, how is it possible that at least for reasonably competent writers producing composition often occurs remarkably fluently, with very few hesitations. The two examples above and similar findings point towards much of the mental activity associated with composition, including the relatively complex processing required to plan sentences, occurring as a result of a cascade of processes that occur partly in parallel and largely without executive control. Consistent with general trends in language processing theory [@dell2007case; @bock2014syntactically; @chang2006], several researchers have argued that the processes associated with written production run at least in part in parallel [@bonin2012evidence; @crump2010hierarchical; @olive2014toward; @van1991handwriting; @roux2013interaction]. @van1991handwriting, for example, argued for a cascade of modular processes, each responsible for a specific transformation (semantic, syntactic, and so forth), with processing occurring as soon as information from the immediately upstream process becomes available [@christiansen2016now]. Buffers provide transient storage at each processing level to accommodate unsynchronised output rates: when lower level processes lag behind higher level processes, buffers allow our fingers to catch up with thought and language related processes. In other words, when writers move from one sentence to another without hesitation this is due to planning the next sentence, to some extent, while completing output of the previous sentence.


This cascaded account of the composition process gives a rather different understanding of inter-keystroke intervals. IKIs result from one of two data-generating processes. When upstream processes output at a rate equal to or faster than can be used for finger movement planning then IKIs are determined just by time needed for executing finger movements (i.e. just by processing below the last buffer in the processing cascade). However if one or more upstream processes provide output more slowly then IKIs become dependent directly on upstream processing times and not on finger movement. IKIs therefore form two distributions, one associated with rapid, fluent output, and another that forms as a result of delays caused by some combination of processing at the semantic, syntactic, lexical, or orthographic levels [@roeser2021modelling].


For example, @roeser2021modelling demonstrated -- using a similar approach as used in the present paper -- how this mixture of distributions that results from the same underlying data generating process can be implemented using Bayesian statistical mixture modelling. In particular they analysed a large sample of copy-task data using commonly used statistical models and compared their out-of-samples predictive performance to a mixture model approach. Not only did they find substantially stronger performance for their mixture model approach but they also demonstrated that mixture models allowed them to capture the process that generates IKIs in a copy-task context in three parameters: (1) the average speed of fluent key transitions, (2) the slowdown for hesitant key presses, and (3) the probability of hesitant as opposed to fluent transitions. As argued by the authors, copy-typing underlies a similar cascade of processes as free text composition. Instead of context generation and grammatical encoding, copy-typing involves a mixture of visual encoding of the target string and mental buffering prior to the activation of motor codes. For similar approaches see  @almond2012preliminary, @baaijen2012keystroke, @chenu2014interword, @guo2018modeling, @hall2022constructing, @li2021identifying, and @waes2019.

The cascading account provides an explanation for why writers sometimes pause before onsetting a sentence (or word) but often are not. This is not because in some cases sentences are not planned, or planning was postponed until after sentence onset, but that this planning was completed in parallel with previous output. 

Hesitations in writing have potential disadvantages. Writers can, as we have noted, pause at any point without this affecting the eventual communicative effect of their text. However language processing is subject to what @christiansen2016now describe as a fundamental "now-or-never" bottleneck. Buffering is transient and as a consequence of written (and spoken) production are just-in-time systems: Production must flow down the cascade of processes from message to finger movements without significant interruptions. If there is delay -- if, for example, a writer struggles to retrieve the right word or its spelling -- there is a risk that output from upstream processes, and particularly the chain of ideas that the writer wanted to communicate, will be lost. @ronneberg2022process coined this possibility the "process-disruption hypothesis".


In the study that is presented in this paper we test using Bayesian statistical hierarchical models of keystroke intervals -- similar to @roeser2021modelling -- whether the cascading view of writing generalises to contexts in which participants compose texts. On the basis of these statistical models of text composition, we appraise to what extent linguistic edges (sentence and word boundaries) are associated with different pausing behaviours as frequently suggested in the literature [@ailhaud2018variations; @ailhaud2016developmental; @conijn2019understanding; @chukharev2019combined; @torrance2016adolescent; @wen02; @mohsen2021second].


Data form spontaneous text production [see @gernsbacher1995coherence] are an ideal test case for our modelling approach for the following three reasons: First, text production such as essay, argumentative, and narrative writing in response to a writing brief or topic statement is a natural environment for text production. Therefore our models have real-world relevance for educators, for example, to evaluate to what extent a student is using a desirable composition strategy [@dux2022automating;@dux2021effect;@vandermeulen2023writing]. 

Second, since the emergence of software such as InputLog [@leijten2013keystroke] recording keystroke data during free text composition has become widely used among writing researchers and led to a rapidly expanding timecourse literature with a focus on the orchestration of subprocesses; with a recent special issue in Reading and Writing [@conijn2021timecourse]. Social Science Citation Index reports 40 journal papers that describe research exploring composition processes using keystroke logging methods in 2022, compared to 26 in 2017 to 2019, and 7 in 2014 to 2016. These data are therefore important for contemporary theories of writing and freely available for modelling and machine learning [@conijn2019understanding; @conijn2020keys].

Third, data from spontaneous text production allow us to address questions about sentence production in context that would not be possible with data from, for example, picture description experiments [@not10; @dam09; @roeser2018advance] or copy tasks [@roeser2021modelling; @waes2019; @van2019multilingual]. In text composition, content generation is not artificially constrained to individual sentence units (or less) by one or more images -- as in sentence-elicitation studies -- and linguistic form is not constraint to a sequence of words -- as in copy tasks. Instead text composition provides data from both inter and intra-sentence transitions. Also, pauses do not reflect visual encoding of the stimulus (although possibly looking back during writing which we will return to in the discussion). 

The hypotheses are as follows: if the preparation of the upcoming production unit happens entirely at the corresponding linguistic edge as predicted by the serial account, and not in parallel to production, key-transition intervals can statistically be modelled as a function of the associated transition location. In other words, we expect IKIs to be proportionally longer at before-sentence locations compared to before-word location compared to within-word locations. This is because transitions before larger linguistic units are associated with processing that involves higher levels of representation. The cascading view, in contrast, assumes that although IKIs at larger linguistic edges might be longer, it is in principle possible that writers do not pause but plan in parallel to writing. In other words, statistical models of the pause-and-burst view of written production do not capture that planning can operate, to some extent, in parallel to the output of the written production. Therefore, a statistical model that captures writing as a cascade of processes must account for the possibility that hesitations occure probabilistic across the entire text although some linguistic locations such as larger linguistic edges are associated with a higher pausing probability. We hypothesize that the statistical model of the parallel cascading view provides a better out-of-sample generalization than the implementation of the serial model of writing in the context of unconstrained text composition.



# Methods

We (re)analysed keystroke data of 6 experiments in which participants composed text in a series of 5 Bayesian models. The first three models map onto the serial account of writing hesitations and bridge between commonly used statistical models for the analysis of inter-keystroke intervals (IKI) as single distribution models. In particular we used a Gaussian distribution, a log-normal model, and an unequal variance model that takes into consideration that longer latencies -- those associated with larger linguistic edges -- are known to be associated with a larger variance component [@wagenmakers2007linear; @wing1973response; @schoner2002timing]. 

The remaining two models are implementations of the cascading view following the two-distributions mixture-model approach presented in @roeser2021modelling. Importantly these two models assume that IKIs result from a mixture of two processes: (1) uninhibited activation flow into motor programmes and (2) interruptions at higher levels cause delays in the information flow. Importantly these two models assume that hesitations are not sufficiently determined by transition location -- as the serial account does -- but also transition locations are associated with different probabilities that hesitation may occur. This view was implemented as a constrained and an unconstrained model. 

We reanalysed datasets from six experiments in which researchers elicited full texts in response to writing prompts. We discussed advantages with this approach in the introduction section. These datasets include samples from populations with different writing experience and languages (e.g. young / L2 writers, students) and writing tasks (e.g. essays, syntheses). Datasets from a variety of different writers and writing tasks were deliberately chosen to challange our modelling approach and to test to what extent resulting pausing patterns generalise across writing contexts.


## Data sets

Six datasets with keystroke data from free text production were used for analysis. An overview can be found in Table \ref{tab:datasets}; details will be presented below.


\blandscape
```{r datasets, results='asis'}
tibble(Dataset = c("C2L1", "CATO", "SPL2", "PLanTra", "LIFT", "GUNNEXP2"),
                Source = c("RÃ¸nneberg et al. (2022)",
                           "Torrance et al. (2016)",
                           "Torrance et al. (n.d.)",
                           "Rossetti and Van Waes (2022b)",
                           "Vandermeulen, Steendam, et al. (2020)", 
                           "Torrance and Ofstad (n.d)"),
                Keylogger = c("EyeWrite", 
                           "EyeWrite", 
                           "CyWrite", 
                           "InputLog", 
                           "InputLog",
                           "EyeWrite"),
                `Writing task` = c("Argumentative essays", 
                                 "Expository texts",
                                 "Argumentative essays",
                                 "Text simplification",
                                 "Synthesis", 
                                 ""),
                 `N (ppts)` = as.character(c(126, 26*2, 39, 47, 658, 45)),
#                n_texts = c(1, 2, 2, 2, NA),
#                n_sentences = c(),
#                n_words = c(),
                `Mean age` = c(11.8, 16.9, 20.6, 23, 16.95, NA),
                 Language = c( "Norwegian", 
                               "Norwegian", 
                               "English (L1) / Spanish (L2)", 
                               "English (L2)", 
                               "Dutch",
                               "Norwegian"),
                Manipulation = c("--", 
                               "weak decoders / control; masked / unmasked",
                               "write in L1 / L2",
                               "pre / post test trained in plain language principles and control", 
                               "Various topics and genres",
                               "masked / unmasked")) %>% 
  arrange(Dataset) %>% 
  apa_table(align = c(rep("l", 4), rep("r", 2), "p{2cm}", "p{3cm}"), 
            escape = FALSE, 
            font_size = "footnotesize",
            caption = "Datasets in brief.")

```
\elandscape




TODO: Mark, can you add information for C2L1 and CATO as needed

TODO: might need to remove kids that don't speak Norwegian at home (see github issue)? I don't think that's necessary though.

For C2L1, CATO, and GUNNEXP2, keystroke data were captured using EyeWrite [@sim07; @torrance201203].

The C2L1 dataset comprises data Norwegian 6th graders -- *N* = 126, mean age 11 years 10 months -- published in @ronneberg2022process. The children composed argumentative essays in Norwegian, a language with a relatively shallow orthography. 

The CATO dataset is published in @torrance2016adolescent. Norwegian upper secondary students -- *N* = 26, mean age = 16.9 years -- with weak decoding skills and 26 age-matched controls composed expository texts by keyboard under two conditions: normally and with letters masked to prevent them reading what they were writing. 


TODO: Mark, can you add information here? I've added a provisional bib entry that needs changing or we just say "unpublished". I could also just use Escop as reference (also for SPL2; maybe my SIG Writing talk). Was the sample Norwegian?

The GUNNEXP2 dataset published in @torranceb includes keystroke data from a text composition task performed by Norwegian undergraduate students (mean age: XXX). In this dataset participants wrote texts either in a masked condition in which the produced text was replaced by 'x's or in an unmasked condition in which the students composed text as normal. 


The SPL2 data set involves undergraduate university students in the US -- *N* = 39, 2mean age = 20.6 years, SD = 1.51 -- composing argumentative essays in English (L1) and Spanish (L2). Students wrote within the CyWrite word-processing environment [@chukharev2019combined]. Participants wrote, with a 40 minute time limit, to two writing prompts. Order and writing prompt were counterbalanced across language. 


LIFT and PLanTra data data were captured using InputLog [@leijten2013keystroke; @van2019multilingual; @waes2019]. 

The LIFT data are published in @vandermeulen2020 and described in @vandermeulen2020mapping. LIFT is a representative sample of Dutch students (*N* = 658, mean age = 16.95 years, 428 females and 230 males) in the three highest grades of pre-university education collected from 43 schools. The students were asked to conduct four synthesis writing tasks, two argumentative and two informative texts, about each of four topics, with order randomized per school. The students received 50 minutes for each tasks. 

<!-- It should be pointed out that, while some students decided to revise the assigned texts, the majority of them opted for rewriting the texts from scratch. -->

The PLanTra (Plain Language Training for business content) data are published in @rossetti2022text and described in @rossetti2022s. The primary aim of the project was to investigate the impact of plain language instruction on business students' strategies to simplify business texts as well as on the comprehensibility of the produced texts. A total of *N* = 47 graduate students (mean age = 23 years, 38 females and 9 males, 45 native Dutch speakers) of the master Business and Economics participated. The study adopted a pre-test post-test design. As pre-test, participants were asked to rewrite a given text (extract of a corporate report on sustainability), to make it more engaging and easier to read for a lay audience. Thereafter the experimental group received online instruction on how to apply plain language principles to sustainability content, while the control group received online instruction exclusively on the topic of sustainability. Participants were asked to spend at least 45 minutes on the instruction module. As post-test, participants were asked 2-3 days later to simplify another extract of a corporate report on sustainability. Both reports were written in English (second language) and similar in length (274-278 words) and readability. Participants received as much time as needed for each task. 





TODO: Mark, can you add information here?

The SPL2 dataset is published in @torrancea. The data come from a text composition task of undergraduate university students -- *N* = 39, 28 female, mean age = 20.6 years (SD = 1.51) -- who wrote two short argumentative essays, one in English (the student's first language in all cases; L1) and one in Spanish (L2) using CyWrite [@chukharev2019combined]. Participants wrote essays in response to each of two prompts, with order and L1 / L2 counterbalanced across subjects.




## Transition types

The transition types that were analysed in this study focus on those locations that were found, by previous research, to be psycholinguistically meaningful [@torrancea; @chukharev2019combined; @torrance2016adolescent; @de2018exploring] and are detailed in Table \ref{tab:keyloc}. In particular we analysed the key-transitions that resulted in the insertion of a character that started a new sentence as before-sentence transition; transitions that started a new word other than those at the beginning of a sentence were treated as before-word transition; all transitions within a word but not the key-transition between the last letter of a word and the subsequent space or punctuation mark were treated as within-word transitions. At before-sentence locations, IKIs were timed to the shift keypress for most data sets (CATO, C2L1, SPL2, GUNNEXP2) but included the transition to the following sentence-initial letters in some data sets (PLanTra, LIFT); we will return to this issue in the Results section. Transitions that occurred at the beginning of the text or the beginning of a paragraph were not treated as before-sentence transitions and were removed from the analysis.

```{r keyloc, results='asis', fig.pos="b"}

table <- tibble(`Transition type` = c("Within word", "Below word", "Before sentence"),
       Description = c("Transitions between any letter",
                       "Keypress after space followed by any letter",
                       "Keypress following a space preceding any letter"),
       Example = c("T$^{\\wedge}$h$^{\\wedge}$e c$^{\\wedge}$a$^{\\wedge}$t m$^{\\wedge}$e$^{\\wedge}$o$^{\\wedge}$w$^{\\wedge}$e$^{\\wedge}$d. T$^{\\wedge}$h$^{\\wedge}$a$^{\\wedge}$t[bsp][bsp]e$^{\\wedge}$n i$^{\\wedge}$t s$^{\\wedge}$l$^{\\wedge}$e$^{\\wedge}$p$^{\\wedge}$t.", 
                   "The $^{\\wedge}$cat $^{\\wedge}$meowed. That[bsp][bsp]en $^{\\wedge}$it $^{\\wedge}$slept.", 
                   "The cat meowed. $^{\\wedge}$That[bsp][bsp]en it slept."))

apa_table(table, caption = "Transition location classification.",
            align = c("p{3cm}", "p{3cm}", "p{8cm}"), 
            escape = FALSE, 
            font_size = "footnotesize",
            note = "$'^{\\wedge}$' marks transition location; [bsp] represents backspace.")

```



## Data reduction

```{r reductionfunctions}
source("scripts/get_data_summary.R")

# Get info about data reduction
n_samples <- 100
c2l1 <- get_c2l1("../data/c2l1.csv", n_samples = n_samples)
cato <- get_cato("../data/cato.csv", n_samples = n_samples)
lift <- get_lift("../data/lift.csv", n_samples = 50, n_ppts = 100)
plantra <- get_plantra("../data/plantra.csv", n_samples = n_samples)
spl2 <- get_spl2("../data/spl2.csv", n_samples = n_samples) 
gunnexp2 <- get_gunnexp2("../data/gunnexp2.csv", n_samples = n_samples)
```

For all datasets we only used transitions that were not followed by an editing operation. Transitions that occurred at the beginning of the text or the beginning of a paragraph were removed. We removed participants that did not complete all conditions in studies with within-participant factors (reducing the number of participants to 343 in the LIFT data set, and 41 participants in the PLanTra data set). We removed participants that produced less than 10 sentences (LIFT: 109 participants; PLanTra: 3 participants; SPL2: 1 participant). 

We further removed keystroke intervals that are extremely short ($\le$ 50 msecs) or extremely long ($\ge$ 30 secs); the percentage of remove keystroke data can be found in Table \ref{tab:datareduction}. From the remaining data we randomly sampled `r n_samples` observations per participant, per condition, and per transition location, with the exception of the LIFT data set. This was done for computational reasons to reduce the time the Bayesian models -- decribed in the next section -- require to complete sampling. For the LIFT data set we reduced the number of participants to 100 which is substantially more than most of the other data sets in our data pool. Because the LIFT data set included the large number of writing tasks as fixed effect, we sampled 50 observations per condition, location and participant to not exceed our computational resources. The percentage of keystroke data that went into the final analysis can be found, by transition location, in Table \ref{tab:datareduction}.




```{r datareduction, results='asis'}
c2l1$ds <- "c2l1"
cato$ds <- "cato"
lift$ds <- "lift"
plantra$ds <- "plantra"
spl2$ds <- "spl2"
gunnexp2$ds <- "gunnexp2"

extrem_values <- map_dfr(list(c2l1, cato, lift, plantra, spl2, gunnexp2), ~.x[[1]] %>% 
          mutate(ds = .x$ds)) %>% 
  mutate(across(where(is.numeric), ~round(.*100, 2))) %>% 
  unite("<50ms", starts_with("too_fast"), sep = " (") %>% 
  unite(">30,000ms", starts_with("too_slow"), sep = " (") %>% 
  mutate(across(c(`<50ms`, `>30,000ms`), ~str_c(., ")"))) %>% 
  rename(`$\\le$ 50 msecs` = `<50ms`,
         `$\\ge$ 30 secs` = `>30,000ms`)
  
random_sample <- 
  map_dfr(list(c2l1, cato, lift, plantra, spl2, gunnexp2), ~.x[[2]] %>% 
          mutate(ds = .x$ds)) %>% 
    mutate(across(where(is.numeric), ~round(.*100, 1))) %>%
    unite("keep", starts_with("keep"), sep = " (") %>% 
    mutate(across(c(keep), ~str_c(., ")"))) %>%
  pivot_wider(names_from = location, values_from = keep) %>% 
  relocate(`within word`)

datareduction <- left_join(extrem_values, random_sample, by = "ds") %>% 
  relocate(ds) %>% 
  mutate(across(ds, ~recode_factor(., 
                "c2l1" = "C2L1",
                "cato" = "CATO",
                "gunnexp2" = "GUNNEXP2",
                "lift" = "LIFT",
                "plantra" = "PLanTra",
#                "spl2_shift" = "SPL2 (shift + C)", 
                "spl2" = "SPL2",
                .ordered = T
                ))) %>% 
  arrange(ds) %>% 
  rename(`Dataset` = ds)

apa_table(datareduction, 
          escape = FALSE,  
          placement = "bp!",
          caption = "Data reduction. Mean percentage of extreme data removed and the mean percentage of randomly sampled data by transition location. Standard error is shown in parentheses.",
          align =c("l", rep("r", 5)),
          col_spanners = list(" " = 1,
                              "Extreme values in \\%" = c(2, 3),
                              "Randomly sampled data in \\%" = c(4, 6))) 

```




## Bayesian statitical modelling

We reanalysed keystroke data -- transition durations measured as the time between first and second keypress -- from six datasets in a series of five Bayesian models. An overview of all models can be found in Table \ref{tab:models}. More detailed model descriptions and motivations can be found in Appendix \ref{statistical-models}.

```{r models, results = 'asis'}
models <- tibble(Models = paste0("M",1:5),
       Type = c("LMM", "LMM", "LMM", "MoG", "MoG"),
       `Eq.` = str_c("\ \\ref{eq:", c("unimodgaus", "unimodloggaus", "unimoduv", "bimodcon", "bimoduncon"), "} "),
       Description = c("Standard single Gaussian model with effects of transition location by study-specific manipulations and by-participant random effect",
                       "Single distribution log-Gaussian model equivialent to M1",
                       "Equivialent to M2 but with different variance components for each transition location (unequal variance)",
                       "Two-distributions mixture of a log-Gaussian of fluent intervals and another of hesitant; distribution of hesitant keystroke intervals is equivialent to M3 but distribution of fluent intervals has a mean across all design cell. Mixing proportion captures the relative number of disfluent transitions",
                       "Equivialent to M4 but distribution of fluent transitions is allowed to vary by transition location")) %>% 
  select(Models, Description)


apa_table(models, 
          align = c(rep("l", 1), "p{13cm}"), 
          escape = FALSE, 
          digits = 0,
          caption = "Overview of typing-process models. All models were fitted with random intercepts for participants.")
#          note = "LMM = Single distribution mixed-effects models; MoG = two distributions mixture of log-Gaussians") 
```


Models M1, M2, and M3 are single distribution models -- consistent with the serial account -- include fixed effects for each combination of transition location and dataset specific manipulations and random effects for participants. In particular models M1 and M2 are consistent with standard models used in the literature and therefore serve as baseline models. For model M3 we relaxed the equal variance assumption for transition locations, thus allowing each transition location to assume a different standard deviation (see Appendix \ref{statistical-models} for rationale).

Models M4 and M5 and extensions of the single distribution models. Importantly models M4 and M5 assume that keystroke data are the result of a combination of two data generating processes -- consistent with the cascading account -- rather than one. These are called finite mixture models in the literature [@gelman2014; @roeser2021modelling; @peel2000finite]. 

These models extend the assumption that processing at higher levels of activation lead to longer pauses. Instead of assuming that there is one process that shifts the distribution of IKIs of larger linguistic edges, we introduce the assumption that pauses at larger linguistic edges are more likely but not obligatory. This is achieved by modelling IKIs as coming from a weighted mixture of two distributions associated with two different states: 

1. Activation can flow into keystrokes without interruption. These fluent keystroke transitions are merely constrained by a person's ability to move their finger and will be captured by the $\beta$ parameter illustrated in equation \ref{eq:bimodcon2}. 

2. Interruptions in the activation flow from higher to lower levels result in longer keystroke intervals, when words or their spelling could not be retrieved in time. The slowdown for these hesitant transitions is captured by $\delta$ in the first line of equation \ref{eq:bimodcon2}. 


\begin{equation}
\begin{aligned}
(\#eq:bimodcon2)
\text{iki}_{i} \sim\text{ } & \theta_\text{location[i], participant[i]} \cdot \text{LogN}(\beta + \delta_\text{location[i]} + u_\text{participant[i]}, \sigma_{e'_\text{location[i]}}^2) + \\
  & (1 - \theta_\text{location[i], participant[i]}) \cdot \text{LogN}(\beta + u_\text{participant[i]}, \sigma_{e_\text{location[i]}}^2)\\
\text{where: } & u_\text{participant} \sim \text{N}(0, \sigma_\text{p}^2) \\
\text{constraint: } & \delta, \sigma_{e}^2, \sigma_\text{e'}^2, \sigma_\text{p}^2>0\\
		& \sigma_{e'}^2 > \sigma_{e}^2\\
		& 0 < \theta < 1
\end{aligned}
\end{equation}


The first line of equation \ref{eq:bimodcon2} represents the distribution of hesitant key transitions, and the second line represents fluent key transitions. Each of these two distributions is associated with the mixing weight $\theta$ which is a proportion that is constrained to be larger than 0 and smaller than 1. $\theta$ is here parameterised to represent the probability that an IKI is associated with the distribution of long IKIs. This probability is inversely related to the mixing weight of the distribution of short IKIs by $1-\theta$ . In other words, a larger weight of one distribution inevitably means a lower weight for the other distribution. The weights of both distributions must sum to 1. We call this parameter the probability of hesitant transitions.

The size of a fluent key-transition does not necessarily vary by transition location which is what our constrained model M4 assumes. In other words, the parameter $\beta$ is the same for before-sentence, before-word, and within-word transitions. However, letter bigrams and trigrams may be systematically executed faster than transitions between between space and a letter (REFERENCE?) or complex keystrokes that comprise space and shift-letter combinations for upper case characters before sentences (we will address the latter possibility in the results section). This is because bigrams / trigrams might be stored, retrieved and executed as motor codes but not transitions to a space following a word or between a space or shift key press preceding a letter. We therefore also implemented an unconstrained model M5 that assumes that the distribution over durations of fluent transitions -- represented as $\beta$ -- varies across transition locations.


All models were implemented in the Bayesian framework. The Bayesian framework, as used in this paper, is ideal for the estimation of parameter values. This is because Bayesian parameter estimates are expresses as probability distributions of the uncertainty associated with parameter value estimates [@farrell2018computational; @gelman2014; @lee2014bayesian]. To achieve this, Bayesian models require the explicit inclusion of prior information, i.e. existing knowledge about parameter values. For small data sets priors influence the inferred parameter value estimates (known as the posterior); for larger data sets weakly informative and vague priors are quickly overcome by the data [i.e. automatic Ockham's razor, @jefferys1992ockham]. In other words the choice of priors values has less impact on the posterior. In the present paper, we use weakly informative priors to aid model convergence by constraining the parameter space [see e.g. @lambert2018student; @mcelreath2016statistical]. Also, as the sample size of the reanalysed datasets is large, our weakly informative priors will not affect the posterior in any meaningful way. 

For all models we included transition location (levels: before sentence, before word, within word) as predictor; for detailed descriptions of parameters associated with transition location see equations \ref{eq:unimodgaus} -- \ref{eq:bimoduncon} in Appendix \ref{statistical-models}. Also, we included dataset-specific experimental manipulations as fixed effects for the same model parameters as transition location. Stan code for mixture models was based on @roeser2021modelling [see also @vasishth2017; @vasishth2017feature] and can be found on OSF (ADD URL HERE); also for a tutorial how to fit Bayesian mixture models see ADD URL HERE.^[Data were analysed in Bayesian mixed effects models [@gelman2014; @mcelreath2016statistical]. The R [@R-base] package rstan [@rstan] was used to interface with the probabilistic programming language Stan [@carpenter2016stan] which was used to implement all models. Models were run with 20,000 iterations on 3 chains with a warm-up of 10,000 iterations and no thinning. Model convergence was confirmed by the Rubin-Gelman statistic ($\hat{R}$ = 1) [@gelman1992] and inspection of the Markov chain Monte Carlo chains. The predictive performance of our models was compared using leave-one-out cross-validation [@vehtari2015pareto; @vehtari2017practical; @sivula2020uncertainty].]



# Results

## Model comparisons

To compare the out-of-sample predictive performance of our models we used Pareto smoothed importance-sampling leave-one-out cross-validation [@vehtari2015pareto; @vehtari2017practical]. Predictive performance was estimated as the sum of the expected log predictive density ($\widehat{elpd}$) and compared via its difference $\Delta\widehat{elpd}$ between models. Similar to other cross-validation techniques, the advantage of using leave-one-out cross-validation is that more complicated models -- models with more parameters -- are penalised to prevent overfit.

Model comparison results for all data sets are shown in Table \ref{tab:loos}. For all data sets we found the same pattern. Both two-distributions mixture models provided a substantially better fit than any of the single distribution models. The unconstrained version of the mixture model rendered a higher predictive performance than the constrained version which does not allow the distribution of short keystroke-intervals to vary across transition locations (and data set specific manipulations). Among the single distribution models we found higher predictive performance for the unequal variance model compared to the log-Gaussian model. The weakest model was the single distribution Gaussian model.



```{r}
files <- list.files(
  str_c("../stanout/", 
  c("lift", "spl2", "spl2_shift", "plantra", "cato", "c2l1", "gunnexp2")), 
  pattern = "model.+.csv", 
  full.names = T)

table <- purrr::map_dfr(files, ~read_csv(.) %>% 
                          mutate(dataset = .x)) %>% 
  select(dataset, model, elpd_diff, se_diff, elpd_loo, se_elpd_loo) %>%
  mutate(across(dataset, ~sub(".*/([^/]+)/[^/]+\\..*", "\\1", .)),
         across(where(is.numeric), ~round(., 0)),
         across(where(is.numeric), ~format(., big.mark = ",")),
         across(everything(), ~str_trim(.))) %>%
  filter(dataset != "spl2_shift") %>% 
  transmute(
     across(dataset, ~recode(., 
                "cato" = "CATO",
                "spl2" = "SPL2", 
#                "spl2_shift" = "SPL2 (shift + C)", 
                "plantra" = "PLanTra",
                "lift" = "LIFT",
                "c2l1" = "C2L1",
                "gunnexp2" = "GUNNEXP2")),
    m = recode(model, 
               lmmgaus = "M1",
               lmm = "M2",
               lmmuneqvar = "M3",
               mogbetaconstr = "M4",
               mogbetacontr = "M4",
               mogbetaunconstr = "M5"),
    across(model, ~recode(., lmm = "Single distribution log-Gaussian",
                          lmmgaus = "Single distribution Gaussian",
                          mogbetaconstr = "Two distributions mixture of log-Gaussians (constrained)",
                          mogbetacontr = "Two distributions mixture of log-Gaussians (constrained)",
                          mogbetaunconstr = "Two distributions mixture of log-Gaussians (unconstrained)",
                          lmmuneqvar = "Single distribution log-Gaussian (unequal variance)")),
            across(elpd_diff, ~str_c(., " (", se_diff, ")")),
            across(elpd_loo, ~str_c(., " (", se_elpd_loo, ")"))) %>% 
  mutate(across(elpd_diff, ~str_replace(., "0 \\(0\\)", "--"))) %>% 
  arrange(dataset)

table <- table %>% select(-elpd_loo) %>% 
  pivot_wider(names_from = dataset, values_from = elpd_diff) %>% 
  rename(Model = m, Description = model)


```

\blandscape
```{r loos, results='asis'}
#names(table)[-1] <- c("Model", "Description", rep(c("$\\Delta\\widehat{elpd}$", "$\\widehat{elpd}$"), 1))
#names(table) <- c("Model", "Description", rep(c("$\\Delta\\widehat{elpd}$"), 1))

apa_table(table, 
      caption = "Model comparisons. The top row shows the models with the highest predictive performance compared to all other models. Standard error is shown in parentheses.",
      align =c("p{1.5cm}", "p{5cm}", rep("r", 6)), 
      escape = FALSE,
      longtable = T,
      font_size = "small",
      note = "$\\Delta\\widehat{elpd}$ = difference in predictive performance -- estimated as expected log pointwise predictive density -- relative to the model with the highest predictive performance in the top row.")
#                col_spanners = list(" " = 1:2,
#                     "\\textbf{CL21}" = 1:5,
#                     "\\textbf{CATO}" = 6:10,
#                     "\\textbf{GUNNEXP2}" = 11:15, 
#                     "\\textbf{LIFT}" = 16:20,
#                     "\\textbf{PLanTra}" = 21:25,
#                     "\\textbf{SPL2}" = 26:30))
#      stub_indents = list(
#                     "\\textbf{CL21}" = 1:5,
#                     "\\textbf{CATO}" = 6:10,
#                     "\\textbf{GUNNEXP2}" = 11:15, 
#                     "\\textbf{LIFT}" = 16:20,
#                    "\\textbf{PLanTra}" = 21:25,
#                     "\\textbf{SPL2}" = 26:30))
#                     "\\textbf{SPL2 (shift + C)}" = 31:35)) 

# $\\widehat{elpd}$ = predictive performance indicated as expected log pointwise predictive density;
```
\elandscape


We also evaluated to what extent model predictions fit observed data. These comparisons can be found in Appendix \ref{fit-to-data} and echo the findings reported in the model comparisons in Table \ref{tab:loos}. Model predictions fit the data well in the case of two-distributions mixture models and poorest for the single distribution Gaussian model.


## Transition location effect

As demonstrated in the model comparisons, the two-distributions mixture-model capture the writing-process data better than single-distribution models. Therefore, writing process is captured by the posterior estimates of the model parameter values. There are three conceptually important parameters: (1) the average duration of fluent transitions (which was indicated as $\beta$ in equation \ref{eq:bimodcon2}), (2) the magnitude of the slowdown for hesitant transitions (represented as $\delta$), and (3) the probability of hesitant transition duration (represented as $\theta$). 

Figure \ref{fig:mixmodel} illustrates the mixture model approach: for each transition durations we obtain two posterior distributions, one associated with fluent transitions between keys and another for key transitions where upstream difficulty result in longer durations. The relative weighting of these two distributions was captured as the probability of hesitant transitions, the parameter that captures the height of the distribution of slow keystrokes. 




```{r mixmodel, fig.cap='Mixture model plot for each transition location. Shown are the distributions for both mixture components (fluent and hesitant transition durations) and their respective weighting. Posterior of SPL2 data (English sessions only).', fig.height=6}

ps <- read_csv("../stanout/spl2/mog_constr_posterior_for_densityplot.csv") 
# Extract parameters
alpha <- filter(ps, param == 'beta') %>% pull(est)
alpha2_bs <- filter(ps, param == 'delta', loc == 'before sentence') %>% pull(est)
alpha2_bw <- filter(ps, param == 'delta', loc == 'before word') %>% pull(est)
alpha2_ww <- filter(ps, param == 'delta', loc == 'within word') %>% pull(est)

theta_bs <- filter(ps, param == 'prob',  loc == 'before sentence') %>% pull(est)
theta_bw <- filter(ps, param == 'prob',  loc == 'before word') %>% pull(est)
theta_ww <- filter(ps, param == 'prob',  loc == 'within word') %>% pull(est)

sigma <- filter(ps, param == 'sigma') %>% pull(est)
sigmap_bs <- filter(ps, str_detect(param, 'sigma'), loc %in% c('overall', 'before sentence')) %>% pull(est) %>% sum()
sigmap_bw <- filter(ps, str_detect(param, 'sigma'), loc %in% c('overall', 'before word')) %>% pull(est) %>% sum()
sigmap_ww <- filter(ps, str_detect(param, 'sigma'), loc %in% c('overall', 'within word')) %>% pull(est) %>% sum()

# Data grid
data <- tibble(x = c(0, 20))

colours <- c("Fluent transitions" = "#000000", 
             #"Fluent transitions" = "#56B4E9",
             "Hesitant transitions" = "#E69F00")

# Create density plot
plot_bs <- ggplot(data, aes(x)) +
  stat_function(aes(colour = "Fluent transitions"),
                geom = "line", 
                linewidth = .25,
                fun = plot_mix_comps,
                args = list(alpha, sigma, lam = 1 - theta_bs)) +
  stat_function(aes(colour = "Hesitant transitions"),
                geom = "line", 
                linewidth = .25,
                fun = plot_mix_comps, 
                args = list(alpha2_bs, sigmap_bs, lam = theta_bs)) +
  stat_function(aes(fill = "Fluent transitions"),
                geom = "area", 
                fun = plot_mix_comps, 
                alpha = .25, 
                args = list(alpha, sigma, lam = 1 - theta_bs)) +
  stat_function(aes(fill = "Hesitant transitions"),
                geom = "area", 
                fun = plot_mix_comps, 
                alpha = .25, 
                args = list(alpha2_bs, sigmap_bs, lam = theta_bs)) +
  scale_x_continuous(labels = exp, 
                     breaks = log(c(1, 10, 100, 1000, 10000)), 
                     limits = log(c(10, 50000))) +
  scale_colour_manual(values = colours) +
  scale_fill_manual(values = colours) +
  labs(y = "", x = "", 
       subtitle = "Before-sentence transitions",
       colour = "Mixture\ncomponent",
       fill = "Mixture\ncomponent") +
  theme(legend.position = "right",
        legend.justification = "top")

plot_bw <- ggplot(data, aes(x)) +
  stat_function(aes(colour = "Fluent transitions"),
                linewidth = .25,
                geom = "line", 
                fun = plot_mix_comps,
                args = list(alpha, sigma, lam = 1 - theta_bw)) +
  stat_function(aes(colour = "Hesitant transitions"),
                geom = "line", 
                linewidth = .25,
                fun = plot_mix_comps, 
                args = list(alpha2_bw, sigmap_bw, lam = theta_bw)) +
  stat_function(aes(fill = "Fluent transitions"),
                geom = "area", 
                linewidth = .25,
                fun = plot_mix_comps, 
                alpha = .25, 
                args = list(alpha, sigma, lam = 1 - theta_bw)) +
  stat_function(aes(fill = "Hesitant transitions"),
                geom = "area", 
                fun = plot_mix_comps, 
                alpha = .25, 
                args = list(alpha2_bw, sigmap_bw, lam = theta_bw)) +
  scale_x_continuous(labels = exp, 
                     breaks = log(c(1, 10, 100, 1000, 10000)), 
                     limits = log(c(10, 50000))) +
  scale_colour_manual(values = colours) +
  scale_fill_manual(values = colours) +
  labs(y = "Density", x = "", 
       subtitle = "Before-word transitions",
       colour = "Mixture\ncomponent",
       fill = "Mixture\ncomponent") +
  theme(legend.position = "none")

plot_ww <- ggplot(data, aes(x)) +
  stat_function(aes(colour = "Fluent transitions"),
                geom = "line", 
                fun = plot_mix_comps,
                linewidth = .25,
                args = list(alpha, sigma, lam = 1 - theta_ww)) +
  stat_function(aes(colour = "Hesitant transitions"),
                geom = "line", 
                fun = plot_mix_comps, 
                linewidth = .25,
                args = list(alpha2_ww, sigmap_ww, lam = theta_ww)) +
  stat_function(aes(fill = "Fluent transitions"),
                geom = "area", 
                fun = plot_mix_comps, 
                alpha = .25, 
                args = list(alpha, sigma, lam = 1 - theta_ww)) +
  stat_function(aes(fill = "Hesitant transitions"),
                geom = "area", 
                fun = plot_mix_comps, 
                alpha = .25, 
                args = list(alpha2_ww, sigmap_ww, lam = theta_ww)) +
  scale_x_continuous(labels = exp, 
                     breaks = log(c(1, 10, 100, 1000, 10000)), 
                     limits = log(c(10, 50000))) +
  scale_colour_manual(values = colours) +
  scale_fill_manual(values = colours) +
  labs(subtitle = "Within-sentence transitions", 
       y = "", x = "Transition duration [in msecs]",
       colour = "Mixture\ncomponent",
       fill = "Mixture\ncomponent") +
  theme(legend.position = "none") 

plot_bs / plot_bw / plot_ww 


```



The predictive performance of the unconstrained mixture model is substantially higher than the predictive performance of the constrained mixture model. However, the constrained mixture model is the theoretically more parsimonious model of the cascading view; in other words, differences between transition locations for the component of fluent transitions were to some extent speculative and unexpected. We therefore present the posterior parameter values for the constrained two-distributions mixture model in the following with reference to the unconstrained implementation.


```{r}
files <- list.files(str_c("../stanout/", 
                          c("lift", "spl2", "plantra", "cato", "c2l1", "gunnexp2")), 
                    pattern = "mogbetaco.+.csv", 
                    full.names = T)

ps <- purrr::map_dfr(files, ~read_csv(.x) %>% 
  mutate(data = .x)) %>% 
  mutate(across(data, ~sub(".*/([^/]+)/[^/]+\\..*", "\\1", .)),
         across(data, ~recode_factor(., 
                "cato" = "CATO",# (non-dyslexic, unmasked)",
                "gunnexp2" = "GUNNEXP2",# (unmasked)",
                "spl2" = "SPL2", # (L1)", 
#                "spl2 (shift + C)" = "SPL2 (L1; shift + C)", 
                "plantra" = "PLanTra",
                "lift" = "LIFT",
                "c2l1" = "C2L1",
                .ordered = TRUE))) %>% 
  filter(!(str_detect(data, "SPL2") & lang == "ES" & !is.na(lang)),
         !(str_detect(data, "CATO") & group == "dyslexic" & !is.na(group)),
         !(str_detect(data, "CATO") & task == "masked" & !is.na(task)),
         !(str_detect(data, "GUNNEXP2") & xn == "masked" & !is.na(xn))) 

ps_beta <- filter(ps, param == "beta") %>% 
  rename(beta = value) %>%
  mutate(idx = 1:n(), .by = data) %>% 
  select(idx, beta, data) 

ps <- filter(ps, param != "beta")  %>% 
  pivot_wider(names_from = param, 
              values_from = value) %>% 
  unnest(cols = beta2:theta) %>%
  mutate(idx = 1:n(), .by = c(data, location, task, xn, genre, topic, lang)) %>% 
#  summarise(across(idx, list(max = max)), .by = c(location, data, task))
  left_join(ps_beta, by = c("data", "idx")) %>%
  mutate(across(c(beta, beta2), ~exp(.)),
         delta = beta2 - beta,
         across(location, ~str_replace(., " ", "\n"))) %>% 
  pivot_longer(c(beta, beta2, delta, prob)) %>% 
  summarise(across(value, 
                   list(mean = mean, 
                        lower = lower, 
                        upper = upper),
                   .names = "{.fn}"), 
            .by = c(data, location, name)) %>% 
  filter(name %in% c("beta", "beta2", "delta", "prob"))
```

We present the mixture-model posterior estimates for each of the three parameters in the three facets of Figure \ref{fig:crossstudypost2}. Although models were fitted with all data set-specific condition, we aggregated the posterior across conditions^[We aggregated across pre-post test for the PLanTra dataset as well as genre and topic of the LIFT data set. We demonstrate in Appendix \ref{pre-post-test-plantra} and \ref{genre-effect-lift} respectively that there is negligible evidence of differences between these conditions.], and removed conditions that might conflate comparisons^[We removed the masked writing condition in the GUNNEXP2 and CATO, the dyslexic group in the CATO data set, and L2 writing in the SPL2 data set. While there was evidence for language-specific effects in Appendix \ref{l2-effect-spl2}, evidence for marking effects was weak as shown in Appendix \ref{masking-effect-cato-gunnexp2}.]. For posteriors of all conditions within data sets see Appendix \ref{posterior-parameter-estimates}. The resulting posterior allows us to examine differences between transition locations for each data set associated with each of the slowdown duration for hesitant transitions and the hesitation probability. Note that the estimate for fluent transitions was estimated by dataset.

```{r crossstudypost2, fig.cap="Mixture model parameter estimates across studies. Distributions of parameter estimates are represented as posterior mean and 95\\% probability interval (PI). Estimates for the CATO dataset were calculated for the non-dyslexic group, unmasked condition; also the GUNNEXP2 estimtes represent the unmasked condition; SPL2 estimates are for the L1 group."}

posd <- position_dodge(.5)
dotsize <- 2.75
grouplabel <- "Data set:"
shapes <- c(1, 4, 6, 7, 8, 9, 11)
width <- 0
linewidth <- .35
nrow <- 1
beta <- filter(ps, name == "beta", 
               location == "before\nword") %>%
  mutate(name = beta_label,
         location = "overall") %>% 
  ggplot(aes(y = mean, 
             ymin = lower, 
             ymax = upper,
             x = location,
             shape = data,
             group = interaction(data))) +
  geom_line(linewidth = linewidth, 
            alpha = .75, 
            position = posd) +
  geom_errorbar(width = width, alpha = .75, position = posd) +
  geom_point(aes(colour = data), 
            size = dotsize, 
            position = posd) +
  facet_grid(~name, scales = "free_x") +
  scale_shape_manual(values = shapes) +
  scale_colour_colorblind() +
  scale_y_continuous(labels = scales::comma) +
  theme(axis.title = element_blank()) +
  labs(colour = grouplabel,
       shape = grouplabel) +
  guides(colour=guide_legend(nrow=nrow,byrow=TRUE),
         shape = guide_legend(nrow=nrow,byrow=TRUE))

delta <- filter(ps, name == "delta") %>% 
  mutate(name = delta_label,
         across(c(mean, lower, upper), ~./1000)) %>% 
  ggplot(aes(y = mean, 
             ymin = lower, 
             ymax = upper,
             x = location,
             shape = data,
             group = interaction(data))) +
  geom_line(linewidth = linewidth, alpha = .75, position = posd) +
  geom_errorbar(width = width, alpha = .75, position = posd) +
  geom_point(aes(colour = data), 
            size = dotsize, 
            position = posd) +
  facet_grid(~name) +
  scale_shape_manual(values = shapes) +
  scale_colour_colorblind() +
  scale_y_continuous(labels = scales::comma) +
  labs(colour = grouplabel,
       shape = grouplabel) +
  theme(axis.title = element_blank()) +
  guides(colour=guide_legend(nrow=nrow,byrow=TRUE),
         shape = guide_legend(nrow=nrow,byrow=TRUE))


theta <- filter(ps, name == "prob") %>% 
  mutate(name = theta_label) %>% 
  ggplot(aes(y = mean, 
             ymin = lower, 
             ymax = upper,
             x = location,
             shape = data,
             group = interaction(data))) +
  geom_line(linewidth = linewidth, alpha = .75, position = posd) +
  geom_errorbar(width = width, alpha = .75, position = posd) +
  geom_point(aes(colour = data), 
            size = dotsize, 
            position = posd) +
  facet_grid(~name) +
  scale_shape_manual(values = shapes) +
  scale_colour_colorblind() +
  scale_y_continuous(labels = dezero_plot, limits = c(0, 1)) +
  labs(colour = grouplabel,
       shape = grouplabel) +
  theme(axis.title = element_blank()) +
  guides(colour=guide_legend(nrow=nrow,byrow=TRUE),
         shape = guide_legend(nrow=nrow,byrow=TRUE))

plot <- beta + delta + theta + plot_layout(guides = "collect",
                                           widths = c(.5, 1, 1))

grid.arrange(patchworkGrob(plot), 
             left = "Posterior estimate with 95% PIs",
             bottom = "Transition location")

```

In the following we evaluate differences between transition locations for all three mixture model parameters. Figure \ref{fig:crossstudypost2} shows largely the same patterns (with caveats) for keystroke-interval estimates by transition location across data sets for all three mixture model parameters. Full results of these pairwise comparisons can be found in Appendix \ref{transition-location-effect-constrained-mixture-model}. We found that hesitations appear more frequently -- higher hesitation probability -- at before-word transitions than within words across data sets (all BFs > 100). Hesitations were more likely at before-sentence locations compared to before-word locations only for 2 out of 6 datasets (GUNNEXP2 and SPL2, both BFs > 100; but C2L1 and CATO: BFs = 0.7; PLanTra: BF = 0.3; LIFT: BF = 1.6). Hesitations were longer at before-sentence transitions compared to before-word transitions (all BFs > 100, except for C2L1: BF = 1.11 and LIFT = 0.5); before-word transitions were longer compared to within-word transitions for half of the data sets (GUNNEXP2: BF = 27; PLanTra: BF = 56; SPL2: BF = 9.1) but not for the other (C2L1: BF = 0.1; CATO: BF = 0.2; LIFT: BF = 0.4). 
 
To reiterate, for most datasets we found negligible evidence supporting the idea that writers pause more frequently at before-sentence boundaries compared to before-word locations (except for datasets SPL2 and GUNNEXP2). This is interesting because it is generally believed that pausing behaviour is associated with syntactic edges such that more and longer pauses are predicted for key transitions at larger syntactic edges following the pattern before-sentence $>$ before-word $>$ within-word. In fact, LIFT showed some indication that pauses were less likely before sentences than words. In brief, while pauses tend to be longer before sentences and less likely at mid-word locations, it is less clear in which contexts pauses are (1) longer before than within words and (2) more likely to appear before sentences than before words.

The duration of fluent durations was not distinguished by transition location in the constrained version of the two distribution model. Figure \ref{fig:crossstudypost2} highlights that two datasets (C2L1, CATO) show substantially longer fluent transition durations. This is presumably reflecting that the population that this sample is from was the youngest and least experienced writers in our data pool. Hesitation duration and frequencies were similar to other data sets.


Inconsistencies between datasets could, to some extent, be explained on the basis that before-sentence transitions were sometimes defined as involving complex key combination involve the mean or sum of transitions between space, shift and / or the sentence-initial letters. In particular, before-sentence transitions in some  datasets (PLanTra, LIFT) included the character key following the shift key but others did not (CATO, C2L1, SPL2, GUNNEXP2). Appendix \ref{key-combination-effect-constrained-mixture-model} addressed this possibility for the SPL2 dataset and showed that including the character following the shift key substantially increases the hesitation probability (BFs > 100) but it did not affect the duration of fluent transitions or hesitations (BFs < 0.3). However this is in conflict with the earlier finding that before-sentence hesitations are not more likely than before-word hesitations for the datasets that included the character following shift (PLanTra, LIFT). In other words, it is unlikely that patterns in our results can be explained on the basis of how before-keystroke transitions were operationalised (complex key-combinations at before-sentence locations).

The predictive performance of the unconstrained model was higher compared to the constrained model (see Table \ref{tab:loos}). We, therefore, summarize briefly differences in the results for the transition-location effects. Results for the unconstrained version of the mixture model can be found in Appendix \ref{transition-location-effect-unconstrained-mixture-model}. The main difference between the unconstrained and the constrained model is that the latter does not allow the parameter that captures fluent transition durations to vary by transition location or otherwise. Instead it represents the lower bound of keystroke activity for uninhibited activation flow. Higher predictive performance for the unconstrained model suggests variability across transition locations for fluent transition durations, in particular for before-word transitions compared to mid-word transitions (all BFs > 100) but not for before-sentence transitions compared to before-word transitions (all BFs < 0.08, but SPL2 and GUNNEXP2: BFs > 100). The pattern for the hesitation probability was generally similar in both the constrained and unconstrained model. However, for hesitation durations the unconstrained model showed evidence for longer durations at before-word locations compared to within-word location in all but one dataset (all BFs > 10, except for LIFT: BF = 0.7); the hesitation duration pattern for before-sentence transitions compared to before-word transitions remained the same. 


<!-- TODO: address difference between posteriors of shift analysis for constrained and unconstrained model 
- possibly not relevant: there are difference between the posteriors but that's not important for the differences between data sets
-->

# Discussion

The classical serial view [@flower1980ynamics] characterizes writing as a sequence planning and execution cycles that results in a writing process that consists of bursts interrupted by pauses. Our cascading model of writing, in contrast, captures that planning, at least in competent writers can occur in parallel to production. We used Bayesian statistical models, in particular single-distribution and two-distributions hierarchical models, to directly compare the serial and the cascading model of writing, respectively. On the basis of 6 data sets with key data from free text production tasks, we presented compelling and consistent statistical evidence in favour of the cascading view. 

Patters observed in the parameter estimates are generally similar across data sets (with caveats). Under the cascading view of writing, key-transitions that directly precede sentences or words are not necessarily associated with a pause but writers might plan in parallel to writing. On the basis of parameter estimates of the mixture model we tested to what extent larger linguistic edges are associated with the preparation of upcoming planning unit. We found that in 4 of 6 data sets, pauses were as likely to occur before sentences as words, in experienced and novice writers, while mid-word pauses were rare. Also, with few exceptions, pauses at before-sentence locations were associated were longer compared to pauses at before-word transitions (which were longer than the few mid-word pauses). In other words, when writers pause, the duration of their pause generally suggest that larger linguistic edges are associated with higher-level processing. However, the pausing probability results can only be explained by the cascading view of writing, not by the serial view. This is because the hesitation probability results suggest that writers often do not pause before sentences; indeed,  they frequently plan the next sentence in parallel production resulting in roughly identical hesitation probabilities at before-sentence and before-word locations.


Interestingly even the youngest sample in our data pool (C2L1) showed evidence of parallel planning. For example, @olive2014toward described a cascading model that is operating in a serial fashion for inexperienced or struggling writers. This is because of task demands reduce the ability to plan in parallel to processing which leads to a separation of planning units that therefore operate in a serial fashion. However, the results presented in this paper suggest that pausing behaviour in young writers largely mirror results from more experienced writers with the notable exception that pause durations are similar before words and sentence indicating a tendency to prepare utterances in small lexically-based planning units possibly smaller than the clause. The main differences for the young writers was their fluent writing execution is substantially slower compared to more experienced writers in the sample. Also for L2 writers (PlanTra, SPL2) we observe pausing patterns that do not resemble a serialised writing process.



We did observe four difference between data sets that are worth highlighting. These can largely be explained on the basis of difference in sample population (writing competence / experience) and text genre [see also @conijn2019understanding]: 

First, key-transitions of the youngest sample (C2L1) were substantially longer compared to all other datasets with a larger slowdown before words. Yet the overall pausing probability showed a patterns comparable to most other data sets. In particular, a larger pause probability was observed for transitions before sentences / words compared to mid-word transitions. The second and third exceptions concern the GUNNEXP2 and SPL2 data sets:  pauses were more likely to occur at before-sentence transitions compared to before-word transitions while in the remaining data sets hesitations were equally probably at before-word and before-sentence locations (compared to mid-word locations). **TODO: I don't have an explanation for this.** Also, fluent transitions were longer at before-sentence locations compared to before-word locations while in the remaining data sets fluent key-transitions were always shorter at mid-word compared to before-words transitions. This shows a general tendency to slow down tying at sentence boundaries for the GUNNEXP2 and SPL2 samples which was pronounced in the other data sets. Fourth, pauses were longer before sentence compared to before words compared to mid-word paues with two exceptions: (1) no difference was found for before-sentence compared to before-word transitions in C2L1 possibly due to a a word-level planning strategy in this sample of young writers as inexperienced writers tend to use a more localised strategy for text production. (2) pause durations did not vary by transition location in the LIFT data set although the posterior showed the same general patterns as observed in most data sets. The absence of a difference could be explained as specific to the synthesis writing task [**REFERENCE**]. 


A possible concern with our results -- substantially better predictive performance for two-distributions mixture models -- is that, in principle, as the mixture model has more parameters it might always lead to a better fit. We addressed this concern by using cross-validation techniques for model comparison which is preventing overfitting models. In addition we used a simulation approach to compare a single distribution model and a two distributions mixture model. We simulated two data sets that were either based on a single distribution or two-distributions random data generating process. In other words, this approach allows us to test the predictive performance of our models in a context where we know the true underlying data generating process -- single distribution vs two-distributions -- and we can test whether these models can successfully uncover the true parameter value. Modelling details can be found in Appendix \ref{simulation}. As might be expected, we found, that the two-distribution mixture was successful at uncovering the parameter values of the data generated with a two-distributions mixture models; we observed the same for the single-distribution model of the data generated with a single distribution process. Parameters were not successfully uncovered when we switched model type and data set. Importantly though, cross-validation did not show a higher predictive performance for the mixture model, compared to the single distribution model, when applied to data based on a two-distributions process (but for the two-distributions data). Therefore we can rule out the possibility that the overwhelming statistical support for the two-distributions mixture models can be explained in terms of model overfitting. 


Throughout this paper we assumed that hesitations are indicators of planning upcoming ideas and encoding of linguistic units. Alternatively at least some if not all pauses in the writing process might be the result of reading or looking back into previously produced text. There is a limited amount of work that has used eye tracking to investigate regressive eye movement behaviour during writing [@alamargot2010using; @beers2010adolescent; @chukharev2019combined; @de2018exploring; @torrancea; @van2010reading]. There are at least three reasons why writers look back into their text: (1) reading is necessary for revision (i.e. writers must check if writing goals have been met); (2) using text to cue or reinstating ideas after interruption of writing process; (3) error evaluation: e.g. spelling, grammar. We removed transitions that terminated in an editing operation so the pauses we detected are unlikely to reflect revisions. However, frequencies of lookback during writing follow a similar pattern as the one we observed for pausing: @torrance2016reading reported that lookbacks appear with a frequency of 45% before sentences, 12% before words and 5% within words of which 36% were associated with sustained reading but mostly less patterned forward and backward saccades between words ["hopping", see also @chukharev2019combined]. This is except, look backs are more frequent before sentences compared to words while we found that pause frequencies appear equally often before sentences and words. Finally two of the data sets in our poor (CATO, GUNNEXP2) included a manipulation where the produced text was masked (or not). In a reanalysis we found no substantial differences in pausing behaviour for the masked condition; see Appendix \ref{masking-effect-cato-gunnexp2}. In other words, even though reading is likely be explain some proportion of our results it is overall rare, certainly in samples of university students.  




A practical advantage of our mixture model approach [also @roeser2021modelling] is that we are not required to stipulate interval thresholds as plausible candidate for lower pause bound to handle the complex distribution of latency from spontaneous text production. Such threshold are widely used by writing researchers, a strategy that was inherited from early research in speech production [for review see @rochester1973significance], with studies dating back to at least the mid 1990s [@foulin1998extent]. There is one central problems with this approach: A threshold requires a definition of what passes as a pause [@wen06;@van2016keystroke], i.e. a pause criterion threshold often set to 2 secs [@chanquoy1996writing; @kaufer1986composing; @sullivan2002self; @wen02] or some other lower bound [@chukharev2014pauses; @connelly2012predicting; @leijten2013keystroke]. Even if these thresholds were adjusted relative to factors such as writing medium, experience of writer, and text location of pause [see e.g. @wen06] they would be arbitrary. For example, a sentence-initial pause of 2 s has a very different interpretation from a 2 s pause that occurs before or within a word. Our current understanding of the processes that underlie text production does not provide a strong theoretical basis on which to make this decision. So while a 2 secs threshold undoubtedly captures an interesting distinction -- processing that occurs the range zero to 2 secs is very likely to be qualitatively different from processing that takes more than 2 secs. However the same could be argued for any threshold between perhaps 250 msecs and 10 secs that a researcher might care to choose [@chenu2014interword]. Mixture models provide a principled statistical framework that allows the researcher to model behavioural data that come from a combination of cognitive processes without imposing threshold values [ see also @almond2012preliminary; @baaijen2012keystroke; @hall2022constructing; @li2021identifying].





<!-- 


----
Add to OSF
Work on tutorial and add as link to methods
----


~~~~~
Say how the results don't contradic the reported studies but speaks better to their conclusions 
but the modelling frameword doesn't require us to impose pause thresholds
~~~~~

~~~~
Move from results to discussion?
Open: to what extent are even short latencies longer at sentences. This is obvious for when multiple keystrokes are summed across at sentences boundaries as there are typically three key transitions necessary to start a sentence: (1) .^_ , (2) _^[shift], (3) [shift]^C
We believe the second one is the linguistically meaningful one
I think give Davids papers as examples, if he uses sums instead of means (other studies?)

~~~


neural oscillationsoccur every 153 ms (6.5 hz). Which is kind of the magic number for the short-duration IKI component.

Duprez, J., Stokkermans, M., Drijvers, L., & Cohen, M. X. (2021). Synchronization between keyboard typing and neural oscillations. Journal of Cognitive Neuroscience, 33(5), 887â901. https://doi.org/10.1162/jocn_a_01692

GonzÃ¡lez, N., & Calot, E. P. (2023). Dataset of human-written and synthesized samples of keystroke dynamics features for free-text inputs. Data in Brief, 48, 109125. https://doi.org/10.1016/J.DIB.2023.109125

~~~~
I think the serial account isn't all inconsistent with the mixture model. I don't think the serial account says that pauses must occure on linguistic edges but between bursts which can in principlpe happen anywhere (but why would they be random). Obviously the problem here is that one needs to postulate what a pause is instead of leaving it to the model to work out. The serial account and the cascading account make very similar predictions for inexperienced writers.

Maybe distinguih between a strong version of serial model and a weak version that is basically a chunky cacading model


-->


# Conclusion

In contrast to the serial pause-and-burst view, the cascading view emphasis that writers do not necessarily pause to plan the upcoming language unit but plan in parallel to writing execution. Using the Bayesian framework we implemented models of both the serial model of writing and the cascading view. Model comparisons and the inspection of posterior parameter estimates supported with a cascading view of writing but provided strong evidence against the serial view. This pattern was found to be largely consistent different levels of writing experience and languages (e.g. young / L2 writers, students) and writing tasks (e.g. essays, syntheses) included in our data pool. 


# References
```{r create_r-references, echo=FALSE, include=FALSE}
r_refs(file = "references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
  
<div id = "references"></div>
\endgroup
  

```{r render_appendix, include=FALSE}
render_appendix("appendix_models.Rmd")
render_appendix("appendix_fit2data.Rmd")
#render_appendix("appendix_postparam.Rmd")
render_appendix("appendix_postparam_constr.Rmd")
render_appendix("appendix_shift_constr.Rmd")
#render_appendix("appendix_l2.Rmd")
render_appendix("appendix_l2_constr.Rmd")
#render_appendix("appendix_masking.Rmd")
render_appendix("appendix_masking_constr.Rmd")
#render_appendix("appendix_prepost.Rmd")
render_appendix("appendix_prepost_constr.Rmd")
#render_appendix("appendix_genre.Rmd")
render_appendix("appendix_genre_constr.Rmd")
render_appendix("appendix_transloc.Rmd")
render_appendix("appendix_transloc_constr.Rmd")
render_appendix("appendix_shift.Rmd")
render_appendix("appendix_sim.Rmd")


```



