---
title: "Tutorial: fitting mixed-effects and log-Gaussian mixture models on text-writing data"
author: "Jens Roeser"
date: "Compiled `r Sys.Date()`"
bibliography: ref.bib
csl: apa.csl
link-citations: yes
output:
  rmdformats::readthedown:
    lightbox: true
    gallery: false
    highlight: "pygments"
    toc_depth: 3
    use_bookdown: false

---

```{r, include=FALSE}
library(rmdformats)
library(knitr)
library(tidyverse)
library(rstan)
library(brms)
```

```{r setup, include=FALSE}
# Turn off scientific notation
options(scipen = 999) 

# Chunk defaults
knitr::opts_knit$set(width=90)
knitr::opts_chunk$set(fig.width = 8, 
                      fig.height = 4.5,
                      size = "small",
                      echo = TRUE,
                      comment = NA, 
                      warning = FALSE,
                      message = FALSE)

# Plotting defaults
theme_set(theme_bw() + 
            theme(legend.justification = "top",
                  panel.grid = element_blank()))
```


This is a step-by-step walk-through that describes how to fit a finite mixture model of two log-Gaussian distributions using the statistical program R and the `rstan` package to interface with the probabilistic programming language Stan [@carpenter2016stan; @hoffman2014no; @rstan; @rstan2]. We will compare this mixture model to standard Gaussian and log-Gaussian models implemented in `brms` [@burkner2017brms; @R-brms_b].^[An alternative to writing models in Stan is the R package `brms` which provides a flexible framework to implement mixture models and many other types of probability models [@burkner2017brms; @R-brms_b]. In particular, `brms` has a `mixture` function to specify mixtures of various types of distributions. These could be Gaussian, skewed-Normal, shifted-Normal, ex-Gaussian etc. and combinations thereof. There is a large number of probability models for continuous data that are plausible candidates [for reaction time data see e.g. @matzke2009psychological].] For a rationale of using mixture models for writing process data see @roeser2021.

This guide shows how to fit a mixture model written in Stan to keystroke data to then calculate the differences between transition locations in the text. This tutorial is largely self-contained. To run the code below, the reader only needs to install the required packages. Data and Stan code can be loaded from the code presented below.

We require three packages: (1) `rstan` to use R to interface with Stan for fitting Bayesian models^[Instructions on how to install `rstan` can be found [here]( https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started).]; (2) `tidyverse` for data processing and visualisation.^[Run `install.packages("tidyverse")` for installation.]; (3) `brms` for simple Bayesian models.^[Run `install.packages("brms")` for installation.]

```{r eval = F}
library(rstan)
library(tidyverse)
library(brms)
```


# Preparing the data

First we load a sample of keystroke data. Data are loaded from [GitHub](https://github.com/jensroes/prowrite-mixture-models/) using the URL as below and the `read_csv` function. For brevity we use a random subset. Data are from participants that completed an essay writing task in the context of the Prowrite project. 


```{r}
# Load data
data <- read_csv("https://raw.githubusercontent.com/jensroes/prowrite-mixture-models/main/tutorial/data/sampledata.csv") 
```

We reduced the data to three variables: a participant identifier `participant`, the inter-keystroke intervals `iki`, and the transition location `location` (levels: within-word, before-word, before-sentence).

```{r}
data
```

We reduced the number of keystroke transitions to `n` observations per participant and per location.

```{r}
count(data, participant, location)
```


# Uni-modal mixed-effects models

## Gaussian mixed-effects model

### Model


$$
\text{iki}_i \sim \text{N}(\mu_i, \sigma_\text{e}^2)\\
\mu_i = \beta_\text{location[i]} + u_i\\
u \sim \text{N}(0, \sigma_u) 
$$

```{r}
formula <- bf(iki ~ 0 + location + (1|participant), family = gaussian())
```


### Priors

```{r}
get_prior(formula = formula, data = data)
```

```{r}
prior <- set_prior('student_t(3, 350, 100)', class = 'b')
```


```{r}
mean <- 350
sd <- 100
tibble(x = 0:1000) %>% 
  mutate(df_3 = dstudent_t(x, 3, mean, sd),
         df_10 = dstudent_t(x, 10, mean, sd),
         df_100 = dstudent_t(x, 100, mean, sd)) %>% 
  pivot_longer(-x) %>% 
  mutate(across(name, str_remove, "df_"),
         across(name, factor, levels = c(3, 10, 100), ordered = TRUE)) %>% 
  ggplot(aes(x = x, y = value, colour = name)) +
  geom_line() +
  labs(colour = "Degrees of\nfreedom")
```


### Initiate sampling

For the model to converge we need to run a sufficient number of iterations. 30,000 iterations, as below, are a lot but does not guarantee convergence. We need to test convergence using the model's posterior (see blow). To test whether the model has converged, we need to run the model more than one time (i.e. different chains). These can be run at the same time (in parallel) using more than one core of your computer (three cores below).^[If you want to use R to check how many cores are available on your machine, run `parallel::detectCores()` (you might need to install the package `parallel`).] There is no need to use more cores and chains and using less cores would mean that at least one process has to wait until after other processes are completed. When the model has settled on a parameter values, we should observe that all three streams (chains) overlap. Running long chains (many iterations) is useful to get more accurate parameter estimates. 


```{r}
iterations <- 10000      # No of iterations
warmup <- iterations / 2 # Warmup samples to be discarded
nchains <- ncores <- 3   # No of chains / cores used (one chain per core)
```

### Fitting the model


The following code combines the model, the data, and the prior and returns the posterior. This will take a couple of minutes.

```{r eval = F}
fit_gaus <- brm(formula = formula, 
                 data = data,
                 prior = prior,
                 chains = nchains,
                 cores = ncores,
                 iter = iterations,
                 warmup = warmup,
                 sample_prior = TRUE,
                 inits = 0,
                 seed = 365)
```

Because Bayesian models tend to take more time to run, it makes sense to save the results using `saveRDS` for later re-use.

The output of the model (i.e. the posterior) can be saved, so we don't need to run the model again (which can take a while). The function requires the name of the fitted model `fit_gaus`. We prefer to keep the name of the output similar to the name of the Stan code used and ideally the R script that is running the model so it is easy to find the script that is running which model. The model is stored as compressed ".rda" file.


```{r eval = F}
saveRDS(object = fit_gaus,
        file = "stan/gaussian.rda", 
        compress = "xz")
```

To read the fitted model back into your working environment used `readRDS`.

```{r}
fit_gaus <- readRDS("stan/gaussian.rda")
```

The function `fixef` provide a summary of the model inference showing the posterior estimates as average and 95% probability interval by transition location in msecs.

```{r}
fixef(fit_gaus) %>% round()
```



## Log-Gaussian mixed-effects model

### Model

$$
\text{iki}_i \sim \text{logN}(\mu_i, \sigma_\text{e}^2)\\
\mu_i = \beta_\text{location[i]} + u_i\\
u \sim \text{N}(0, \sigma_u) 
$$

```{r}
formula <- bf(iki ~ 0 + location + (1|participant), family = lognormal())
```


### Priors

```{r}
get_prior(formula = formula, data = data)
```

```{r}
prior <- set_prior('student_t(3, 5, .5)', class = 'b')
```


```{r}
mean <- 5
sd <- .5
tibble(x = seq(3, 7, .01)) %>% 
  mutate(df_3 = dstudent_t(x, 3, mean, sd),
         df_10 = dstudent_t(x, 10, mean, sd),
         df_100 = dstudent_t(x, 100, mean, sd)) %>% 
  pivot_longer(-x) %>% 
  mutate(across(name, str_remove, "df_"),
         across(name, factor, levels = c(3, 10, 100), ordered = TRUE)) %>% 
  ggplot(aes(x = x, y = value, colour = name)) +
  geom_line() +
  labs(colour = "Degrees of\nfreedom") 
```

### Fitting the model


```{r eval = F}
fit_lgaus <- brm(formula = formula, 
                 data = data,
                 prior = prior,
                 chains = nchains,
                 cores = ncores,
                 iter = iterations,
                 warmup = warmup,
                 sample_prior = TRUE,
                 inits = 0,
                 seed = 365)
```

Save the results using `saveRDS`.

```{r eval = F}
saveRDS(object = fit_lgaus,
        file = "stan/loggaussian.rda", 
        compress = "xz")
```


The function `fixef` provide a summary of the inferred posterior estimates as average and 95% probability interval by transition location in log msecs.


```{r echo = F}
fit_lgaus <- readRDS("stan/loggaussian.rda")
```


```{r}
fixef(fit_lgaus) %>% round(1)
```



## TODO Model comparison


ADD here info from slides on how to calculate elpd




To evaluate the predictive performance of the models we can use leave-one-out cross-validation. The out-of-sample predictive performance was determined via Pareto smoothed importance-sampling [@vehtari2015pareto; @vehtari2017practical]. Cross-validation, in contrast to more conventional model-comparison metrics such as $R^2$, prevent overfitting by penalising models with more parameters [see @mcelreath2016statistical; @lambert2018student]. Predictive quality was summarised as the sum of the expected log predictive density (`elpd_loo`; $\widehat{elpd}$). The difference in `elpd_loo` is shown in `elpd_diff` ($\Delta\widehat{elpd}$) with standard error shown in `se_diff`. 

```{r}
loo_fit <- loo(fit_gaus, fit_lgaus)
loo_fit$diffs %>% 
  as.data.frame() %>% 
  round(1)
```
The model comparison shows that the model `fit_lgaus` (the model with the log-Gaussian likelihood) is approximately `r round(abs(loo_fit$diffs[2,1] / loo_fit$diffs[2,2]),0)` times better than `fit_gaus` (try `abs(loo_fit$diffs[2,1] / loo_fit$diffs[2,2])`).



# Bi-modal mixed-effects log-Gaussian mixture model

The model returns population estimates for the average typing speed of fluent keystroke transitions ($\beta$), the slowdown for hesitant transitions ($\delta$) and the frequency of hesitant transitions ($\theta$) for each level of a categorical predictor after taking into account two sources of random error: (1) each participant has an individual fluent typing speed that differs across levels of the categorical predictor; (2) each participant has in individual hesitation frequency that differs across levels of the categorical predictor.


$$
\text{iki}_{i} \sim \theta_\text{location[i]} \cdot \text{LogN}(\beta_\text{location[i]} + \delta_\text{location[i]} + u_i, \sigma_{e'_\text{location[i]}}^2) +\\
  (1 - \theta_\text{location[i]}) \cdot \text{LogN}(\beta_\text{location[i]} + u_i, \sigma_{e_\text{location[i]}}^2)\\
		\text{where}\\
		\delta \sim \text{N}(0,1)\\
		\text{constraint: } \delta > 0
$$



## TODO Mixture model simulation





## Model fit

The data must be transformed into a list to feed them into Stan. The information required from the data is determined in the Stan code and has to do with what we need to estimate the model parameters and how the model is implemented. The exert from the Stan code below shows which information is expected as input (e.g. int is integer), how they are named, and what the smallest (i.e. lower) and largest possible value (i.e. upper) are. The full Stan code can be found at the end of this tutorial.

```{r eval=F, engine='Rcpp'}
// Do not run
// Data chunk of Stan code
data {
  int<lower=1> N;                    // number of observations
  int<lower=1> nS;                   // number of ppts
  int<lower=1, upper=nS> ppts[N];    // ppts identifier
  int<lower=1> nB;                   // number of bigrams
  int<lower=1, upper=nB> bigrams[N]; // ppts identifier
  int<lower=1> K;                    // number of conditions
  int condition[N];                  // condition identifiers
  vector[N] y;                       // outcome
}
```


Depending on the data we might have different numbers of participants and conditions. The Stan code is using indices to be able to fit varying numbers for each. In the code below we use `factor` in combination with `as.numeric` to ensure that there are no empty indices. We create vectors with participant identifiers `ppts` (numbers $1$ through $I$ where $I$ is the number of participants) and a numeric identifier `location` for each transition locations (levels: before-sentence = 1, before-word = 2, within-word = 3).  The returned values are indices for the parameters in the model: For example, for the location identifier, `beta[1]` is the population estimate for non-hesitant typing intervals at before-sentence transition locations, `beta[2]` is same for before-word transition locations; `theta[1]` and `theta[2]` are the disfluency probability on the logit scale for before-sentence and before-word transition locations, respectively.

```{r}
ppts <- pull(data, participant) %>% factor() %>% as.numeric()
condition <- pull(data, location) %>% factor() %>% as.numeric()
```

In the below code, the names on the left side of the arrow must correspond to the names expected in the Stan code (above); names on the right side do not. We assign the participant and location identifiers created above and their maximum values `nS` and `K`, respectively. The keystroke data `iki` are assigned to `y` and the total number of observations is assigned to `N` (number of rows in the data `nrow(data`).

```{r}
data_list <- within( list(), {
  ppt <- ppts  
  nS <- max(ppts) # max no of ppts
  condition <- condition
  K <- max(condition) # no of conditions
  y <- data$iki
  N <- nrow(data)
} )
```

The information in the data list can be viewed using the `glimpse` function.

```{r}
glimpse(data_list)
```


## Load model

The Stan code can be downloaded from [GitHub](https://github.com/jensroes/prowrite-mixture-models/blob/main/tutorial/stan/molg.stan) using `readLines` and `writeLines`. The code will be saved as "molg.stan" (mixture of log-Gaussians) in a directory called "stan" which needs to be available in your current working directory. 

```{r eval = F}
# Create an empty directory called "stan"
dir.create("stan")
# Download Stan code
file <- readLines("https://raw.githubusercontent.com/jensroes/prowrite-mixture-models/main/tutorial/stan/molg.stan")
# Save Stan code to current working directory
writeLines(file, "stan/molg.stan")
```

The Stan model can be loaded and translated into an S4 class object using the `stan_model` function and assign it to the object `molg`. The "molg.stan" file needs to be available in the directory "stan" located inside the current working directory.


```{r eval = F}
molg <- stan_model(file = "stan/molg.stan")
```


This code fits the posterior of a categorical predictor with any number of levels. In other words, the posterior that model returns can be used to calculate simple effects, main effects and interactions of a factorial design. We demonstrate below how to calculate a simple effect. The same logic can be used to calculate main effects and interactions. Also, participant estimates are calculated for both mixture components and the mixing proportions by condition. For including more predictors, or removing, e.g., random error terms, or adjusting priors, the user will have to work directly in the Stan code.

This Stan code used is largely based on @sorensen2016bayesian and @vasishth2017. @sorensen2016bayesian presents a detailed tutorial on how to fit Stan models [see also @lambert2018student].


The model is fitting a number of parameters that are not relevant for our inference. To reduce the size of the R object that contains the posterior we can select the parameters of interest and disregard all parameters that are not relevant for our inference.

```{r}
# Parameters to keep in output
pars <- c("beta",                        # fluent typing
          "delta",                       # disfluency slowdown
          "theta",                       # mixing proportion on logit scale
          "prob",                        # mixing proportion as probability
          "prob_s",                      # by-participant disfluency probability
          "sigma",                       # variance component
          "sigma_diff", "sigmap_e", "sigma_e",  # variance by mixture component
          "sigma_u",                     # variance for random ppt 
          "log_lik",                     # log likelihood (for model comparison) 
          "y_tilde")                     # predicted data 
```


The `sampling` function applies the model to the data using the information specified above (iterations, warmup, chains, cores, parameters `pars` to be saved). `save_warmup` is set to `FALSE` to discard the warmup samples (which are not used for inference anyway) to reduce the size of the posterior. To allow reproducibility (and because Bayesian models involves random number generation) we set the seed. The seed can be any number but using the same number ensures the use of the same random number. Lastly, the control argument was specified with higher values for `adapt_delta` and `max_treedepth`: using higher values here mean the model runs slower but supports more careful parameter estimation. The model is assigned to the variable `m`.

```{r eval = F}
# Fit model
fit_molg <- sampling(molg, 
                     data = data_list,
                     iter = iterations,
                     warmup = warmup,
                     chains = nchains, 
                     cores = ncores,
                     pars = pars, # Parameters to keep.
                     save_warmup = FALSE, # Don't save the warmup samples.
                     seed = 365, 
                     # Improve sampling efficiency
                     control = list(adapt_delta = 0.94,  # default: 0.9
                                    max_treedepth = 12)) # default: 10
```

```{r echo = F}
#sampler_params <- get_sampler_params(m, inc_warmup = FALSE)
#sampler_params_chain1 <- sampler_params[[3]]#colnames(sampler_params_chain1)
#sapply(sampler_params, function(x) mean(x[, "accept_stat__"]))
#sapply(sampler_params, function(x) max(x[, "treedepth__"]))
#inits <- get_inits(m)
#inits_chain1 <- inits[[1]]
#print(inits_chain1)
```


Save the results using `saveRDS`.

```{r eval = F}
saveRDS(object = fit_molg,
        file = "stan/molg.rda", 
        compress = "xz")
```


```{r echo = F}
fit_molg <- readRDS("stan/molg.rda")
```


Running this model will take a while to complete sampling depending on your hardware specifications. The time it took my machine to complete this job can me viewed using `get_elapsed_time`. Therefore it is worth to not use all cores for this to run or to use a dedicated high performance machine.


```{r}
get_elapsed_time(fit_molg)/60 # in mins
```




Inferred posterior estimates shown as average and 95% probability interval can be extracted using the `print` function for each transition location and the three parameter values of interest.


```{r}
print(fit_molg, 
      pars = c("beta", "delta", "prob"), 
      probs = c(.025, .975))
```


## Model comparsion

Similar to above we will compare the three models -- the mixture of log-Gaussians, and the uni-model Gaussian and log-Gaussian models -- using leave-one-out cross-validation. Because the mixture model was fitted directly in Stan and the uni-model models were fitted in `brms`, we need to extract the fit statistics before comparing the models using functions provided by the package `loo`.


```{r}
library(loo)
# Mixture of log-Gaussians
log_lik <- extract_log_lik(fit_molg, merge_chains = F) 
r_eff <- relative_eff(exp(log_lik)) # relative effective sample size
loo_molg <- loo(log_lik, r_eff = r_eff)

# Uni-modal log-Gaussian
loo_lgaus <- loo(fit_lgaus)

# Uni-modal Gaussian
loo_gaus <- loo(fit_gaus)
```

As described above we used leave-one-out cross-validation to compare the mixture of log-Gaussians model to each the uni-modal Gaussian and log-Gaussian model. Their difference is shown in `elpd_diff` ($\Delta\widehat{elpd}$) with standard error shown in `se_diff`. Models are ordered from the model with the highest predictive performance to the lowest (i.e. `elpd_loo`). 

```{r}
loo_comp <- loo_compare(loo_gaus, loo_lgaus, loo_molg)
```

To extract the model comparisons and fit statistics:

```{r}
loo_comp %>% as.data.frame() %>% 
  rownames_to_column("models") %>% 
  mutate(across(models, recode, model3 = "fit_molg"),
         across(where(is.numeric), round, 1))  
```

The model comparison shows that the model `fit_molg` (the mixture of log-Gaussian ) is approximately `r round(abs(loo_comp[2,1] / loo_comp[2,2]),0)` times better than `fit_lgaus` (try `abs(loo_comp[2,1] / loo_comp[2,2])`).




## Model diagnostics

### Convergence

Model convergence can be established in two practically simple techniques. First, we can inspect the chains in trace plots which show the parameter estimate across iterations (here after warmup) for each chain. We will look at the population-level parameters (names are determined in the Stan code) and assign those to `pars`.

```{r}
pars <- c("beta", "delta", "theta")
```

To create trace plots, we can apply the `stan_trace` function to the model `m` and extract the MCMC chains for the parameters in `pars`. The `alpha` argument makes the colours slightly more transparent. If the chains overlap and look like "fat hairy caterpillars", chains have converged on the same target distribution. 

```{r}
# Check convergence
stan_trace(fit_molg, pars = pars, alpha = .5)
```



Second, we can calculate the $\hat{R}$ statistic. Successful convergence is reflected in $\hat{R}$ values smaller than 1.05 [@gelman1992]. $\hat{R}$ is similar to the F statistic: it tells us how much bigger the variability between chains is compared to the variability within chains. A value of $\approx 1$ indicates that the variability is essentially identical signifying convergence. For this we can use the `rhat` function applied to the model and the population parameters. 

```{r}
summary(fit_molg, pars=pars)$summary %>% 
  as.data.frame() %>% 
  rownames_to_column("parameter") %>% 
  mutate(across(Rhat, round, 3)) %>% 
  select(parameter, Rhat)
```


Convergence problems can have many reasons and therefore many solutions: running longer chains (in particular longer warmups), increasing the maximum treedepth and the average acceptance probability (`adapt_delta`), specifying starting values, adjusting priors, constraining parameters, or changing the parametrisation of the model. Severe convergence problems are indicative of a misspecification of the model.


### Posterior predictive check

```{r echo = F, include = F}
(total_samples <- (iterations - warmup) * nchains)
```

The model used the posterior parameter values to simulate hypothetical data sets. This happened for every iteration for every chains. In other words there is a total of `r scales::comma(total_samples)` simulated data sets (and samples per parameter) available to us because

```{r}
(total_samples <- (iterations - warmup) * nchains)
```

We can draw predicted data from the model output and compare these to the observed data. A model that makes reasonable predictions should fit the observed data.

Using the `as.matrix` function we extract a matrix of `y_tilde`, the simulated data. This returns a matrix with the size `total_samples` $\times$ `nrow(data)` (so `r scales::comma(total_samples)` $\times$ `r scales::comma(nrow(data))`). We use the `sample` function to draw `N` randomly sampled hypothetical data sets. The `ppc_dens_oberlay` function from the `bayesplot` package is then mapping the observed data ($y$; thick blue line) to the simulated data ($y_{rep}$; thin lightblue lines).

```{r}
y_tilde <- as.matrix(fit_molg, pars = "y_tilde")   # extract simulated data sets
N <- 50                                            # number of simulations to use
rnd_sims <- sample(total_samples, N)               # created random indices
y_tilde_sample <- y_tilde[rnd_sims,]               # draw N random simulations 
```

```{r}
library(bayesplot)
ppc_dens_overlay(data_list$y, y_tilde_sample) +
  scale_x_continuous(limits = c(0, 3000))
```



# Posterior probability distributions

At the core of Bayesian inference is the posterior probability distribution. For each model parameter we have `r scales::comma(total_samples)` posterior samples that form a posterior probability distribution. These samples are the results of the number of iterations after warmup for all chains.

The distribution of parameter values represents the uncertainty about parameter values given the data. There is a large range of things one can do with a posterior. Below we will focus on summarising parameter estimates, comparing conditions, and extracting by-participant estimates. 

The `names` function can be used to remind us of the parameter names used in the model (we reduced the output to the first 15 parameter names). The meaning of the parameters is described above and is determined in the Stan code. Indices refer to the transition locations (1 = before-sentence; 2 = before-word; 3 = within-word) and to the participant identifier for by-participant parameters (indicated by `_s`).


```{r}
names(fit_molg)[!str_detect(names(fit_molg), "log_lik|y_tilde")]
```


## Parameter-value estimates

The parameter estimates can be visualised using the `stan_hist` function with the model variable `m` and the to-be visualised parameters `pars` as arguments. The function returns a histogram for each other parameter values. Each histogram is the probability distribution of the parameter values indicating which values are more or less probable to be the true parameter value.

```{r}
stan_hist(fit_molg, pars = pars)
```


The posterior samples of the parameter values can be summarised using the `print` function. The `probs` argument requires the lower and upper bound of the probability interval that we are interested in. A lower bound of .025 and an upper bound of .975 gives the 95% probability interval (PI; also called "credible intervals"), i.e. the range that contains the true parameter value with a 95% probability given the data. 

The output summaries the most probable parameter value as mean with its standard error (`se_mean`) and standard deviation (`sd`), the effective sample size (`n_eff`) indicating sampling efficiency and the convergence metric $\hat{R}$ (`Rhat`) we introduced above. 

```{r}
print(fit_molg, pars = pars, probs = c(.025,.975))
```


For the following steps, we will focus on the three parameters that have conceptually interesting interpretations: (1) the average fluent typing speed $\beta$, (2) the disfluency slowdown $\delta$, and (3) the disfluency probability $\theta$.

```{r}
pars <- c("beta", "delta", "theta")
```

The `plot` function shows the posterior probability distribution of the three parameters for before-sentence transitions (indicated as 1), before-word transitions (indicated as 2), and within-word transitions summarised as median and 95% PI.

```{r}
plot(fit_molg, pars = pars, ci_level = .95) # ci = credible interval
```

The values for $\beta$ and $\delta$ are shown on a log-scale. To transform their values back to msecs we can extract the posterior samples using the `as.data.frame` function. We prefer the use of tibble objects. 

```{r}
posterior <- as.data.frame(fit_molg, pars) %>% as_tibble() 
posterior
```

The `pivot_longer` function is transforming the data above to a long format with an additional column for component and the model parameters kept as columns. The `names_pattern` argument is using a regular expression to extract the number in the squared brackets.

```{r}
posterior_long <- pivot_longer(posterior, everything(), 
             names_to = c(".value", "location"),
             names_pattern = "(.*)\\[(.)\\]") 
posterior_long
```

This code is then transforming `beta` and `delta` to msecs using the exponential function `exp` to un-log the values. In order to transform the slowdown `delta` into msecs we need to add `beta` before using the exponential function; we can then subtract beta again. `theta` samples are currently on the logit scale and can be transformed to probabilities using `brms::inv_logit_scaled`. The `recode` function changes the component indices from 1 to "before-sentence", 2 to "before-word", 3 to "within-word".

```{r}
posterior_in_msecs <- mutate(posterior_long,
                             delta = exp(beta + delta) - exp(beta),
                             across(beta, exp),
                             across(theta, inv_logit_scaled),
                             across(location, recode, `1` = "before-sentence",
                                                      `2` = "before-word",
                                                      `3` = "within-word"))
posterior_in_msecs
```


The posterior distribution of the parameter estimates can then be visualised in, for example, histograms.

```{r}
pivot_longer(posterior_in_msecs, beta:theta, names_to = "parameter") %>%
  ggplot(aes(x = value, colour = location, fill = location)) +
  geom_histogram(position = "identity", alpha = .25) +
  facet_wrap(~parameter, scales = "free", labeller = label_parsed) +
  scale_fill_brewer("Transition\nlocation", palette = "Dark2") +
  scale_color_brewer("Transition\nlocation", palette = "Dark2") +
  labs(x = "Posterior parameter estimate")
```


## Difference between transition locations

We can calculate the differences between the transition locations for each parameter. This is giving an indication of whether fluent typing ($\beta$) is influenced by linguistic edges, maybe the size of the disfluency slowdown ($\delta$), or the disfluency probability ($\theta$). 

To determine these differences we change the data format above and create a variable that indicates the parameter and one column for the corresponding values for each transition location. The last line calculates the difference between (1) transitions before sentences and words, and (2) transitions before and within words for each of the three parameters.

```{r}
posterior_diffs <- posterior_in_msecs %>%
  pivot_longer(beta:theta, names_to = "parameter") %>%
  group_by(location) %>% 
  mutate(id = row_number()) %>%
  pivot_wider(names_from = location, values_from = value) %>%
  select(-id) %>% # drop id column
  mutate(diff_sent = `before-sentence` - `before-word`,
         diff_word = `before-word` - `within-word`,
         parameter = paste0(parameter, "[diff]")) %>% 
  select(parameter, starts_with("diff"))

posterior_diffs
```

The difference between the conditions can be summarised using the mean, the 95% PIs and the probability that the difference between the components is small 0 (indicated as e.g. $P(\hat{\beta} <0)$ for the parameter $\beta$; the hat $\hat{.}$ symbol indicates that the value is an estimate of the population parameter value). This summary tells us whether the difference between the consonants task and the LF-bigrams task is different from zero and the most probable value for the difference between tasks.

```{r}
# Create some summary functions
lower <- function(x) quantile(x, .025)
upper <- function(x) quantile(x, .975)
p_diff <- function(x) mean(x < 0)

# Summarise differences
posterior_diffs %>%
  pivot_longer(starts_with("diff")) %>% 
  group_by(parameter, name) %>%      # group by parameters, comparison
  summarise(across(value, 
            list(mean = base::mean,  # mean difference
                 lower = lower,      # 2.5% lower bound of difference
                 upper = upper,      # 97.5% upper bound of difference
                 p_diff = p_diff),   # prob that difference is negative
            .names = "{.fn}")) %>% 
  ungroup() %>% 
  mutate(across(where(is.numeric), round, 2)) %>% # round after 2nd decimal place
  arrange(name)
```

```{r echo=F}
ps <- posterior_diffs %>%
  pivot_longer(-parameter) %>% 
  group_by(parameter, name) %>%
  summarise(across(value, 
            list(mean = base::mean,  
                 lower = lower,      
                 upper = upper,      
                 p_diff = p_diff),   
            .names = "{.fn}")) %>% 
  ungroup() %>% 
  mutate(across(where(is.numeric), round, 2),
         parameter = gsub("\\[diff\\]", "", parameter)) 
```


These differences can be viewed in histograms. The vertical line indicates a difference between tasks corresponding to a value of zero. The area of the histogram to the left of this line corresponds to the probability `p` to observe a value smaller than zero as in the output above. 


```{r}
posterior_diffs %>% 
  pivot_longer(starts_with("diff")) %>% 
  mutate(across(name, recode, diff_sent = "before-sentence vs before-word",
                              diff_word = "before-word vs within word")) %>% 
  ggplot(aes(x = value, colour = name, fill = name)) +
  geom_histogram(position = "identity", alpha = .25) +
  facet_wrap(~parameter, scales = "free", labeller = label_parsed) +
  scale_fill_brewer("Difference", palette = "Dark2") +
  scale_color_brewer("Difference", palette = "Dark2") +
  geom_vline(xintercept = 0, colour = "red", linetype = "dashed", size = .5) +
  labs(x = "Difference between transition locations") +
  theme(legend.position = "top")
```

We observe the following differences that can be written up in a report like this: We observe evidence for a higher probability to pause before sentences which is `r ps %>% filter(parameter == "theta", name == "diff_sent") %>% pull(mean)` (95% PI: `r ps %>% filter(parameter == "theta", name == "diff_sent") %>% pull(lower)`, `r ps %>% filter(parameter == "theta", name == "diff_sent") %>% pull(upper)`) times more likely than before words, which was is `r ps %>% filter(parameter == "theta", name == "diff_word") %>% pull(mean)` (95% PI: `r ps %>% filter(parameter == "theta", name == "diff_word") %>% pull(lower)`, `r ps %>% filter(parameter == "theta", name == "diff_word") %>% pull(upper)`) more likely than within words. 

Similarly, differences were found for the hesitation difference. Hesitations before sentences are `r ps %>% filter(parameter == "delta", name == "diff_sent") %>% pull(mean)` msecs (95% PI: `r ps %>% filter(parameter == "delta", name == "diff_sent") %>% pull(lower)`, `r ps %>% filter(parameter == "delta", name == "diff_sent") %>% pull(upper)`) longer than before words, which were `r ps %>% filter(parameter == "delta", name == "diff_word") %>% pull(mean)` msecs (95% PI: `r ps %>% filter(parameter == "delta", name == "diff_word") %>% pull(lower)`, `r ps %>% filter(parameter == "delta", name == "diff_word") %>% pull(upper)`) longer than within words. 

Interestingly though fluent transitions (i.e. planning and executing motor plans) were only faster within words by `r ps %>% filter(parameter == "beta", name == "diff_word") %>% pull(mean)` msecs (95% PI: `r ps %>% filter(parameter == "beta", name == "diff_word") %>% pull(lower)`, `r ps %>% filter(parameter == "beta", name == "diff_word") %>% pull(upper)`) compared to before-word transitions but there was no difference for before-sentence transitions compared to before-word transitions ($\hat{\beta}$ = `r ps %>% filter(parameter == "beta", name == "diff_word") %>% pull(mean)` msecs, 95% PI: `r ps %>% filter(parameter == "beta", name == "diff_word") %>% pull(lower)`, `r ps %>% filter(parameter == "beta", name == "diff_word") %>% pull(upper)`).




## ROPE

In practice, we rarely believe that only an effect of exactly null is indicating that there is no difference between our groups (which is what a null hypothesis assumes). Instead there are values that are non-different from null; for example a slowdown of 1 msecs is meaningless in most, if not all, contexts. To take this into account we can determine the *region of practical equivalence* [ROPE, @kruschke2010believe; @kruschke2011bayesian; @kruschke2014doing]. We can define the ROPE as the range of values that is _a priori_ considered non-different from a null effect. The ROPE value indicates to what extent the posterior can not rule out a negligible effect. A meaningful difference between groups (or effects more generally) should have a small proportion of posterior samples within the ROPE.

To calculate the ROPE for out differences between two transition locations, we can use the `rope` function from the `bayestestR` package.^[To install `bayestestR` run `install.packages("bayestestR")`.] The `rope` function requires as input a vector with the posterior difference. We can create such a vector from our posterior differences. Let's focus on $\beta_\text{diff}$, the average typing speed for fluent transitions.

```{r}
# Extract vector of differences
# For sentence vs word
beta_diff_sent <- filter(posterior_diffs, parameter == "beta[diff]") %>% pull(diff_sent)

# For before word vs within word
beta_diff_word <- filter(posterior_diffs, parameter == "beta[diff]") %>% pull(diff_word)
```

Also, we need to define the `range` argument which specifies the range of values are not considerably different from null. Of course this range will depend on what the parameter represents and researcher intuition. For example, for fluent typing we would consider small differences, of say 10 msecs, more meaningful than for disfluencies. If we consider differences of 10 msecs or smaller equivalent to a null difference, we can define the `range` argument with a lower bound of -10 and the upper bound of 10.

```{r}
library(bayestestR)
rope(beta_diff_sent, range = c(-10, 10))
```

The ROPE for fluent typing speed for the difference between before-sentence and before-word transitions contains `r round(pull(rope(beta_diff_sent, range = c(-10, 10)))*100,2)`% of the posterior samples. In other words, there is a `r round(pull(rope(beta_diff_sent, range = c(-10, 10)))*100,2)`% probability that the posterior difference $\beta_\text{diff}$ is between -10 and 10 msecs which we considered negligible.


```{r}
rope(beta_diff_word, range = c(-10, 10))
```

However, the ROPE for fluent typing speed for the difference between before-word and within-word transitions contains `r round(pull(rope(beta_diff_word, range = c(-10, 10)))*100,2)`% of the posterior samples. In other words, there is a `r round(pull(rope(beta_diff_word, range = c(-10, 10)))*100,2)`% probability that the posterior difference $\beta_\text{diff}$ is between -10 and 10 msecs which we considered negligible.

Let's see what happens if we determine a wider ROPE for negligible differences. What happens if we define a ROPE of -50 through 50 msecs for the difference between before-word and within-word transitions.

```{r}
rope(beta_diff_word, range = c(-50, 50))
```

The ROPE contains `r round(pull(rope(beta_diff_word, range = c(-50, 50)))*100,2)`% of the posterior samples. In other words, there is a `r round(pull(rope(beta_diff_word, range = c(-50, 50)))*100,2)`% probability that the posterior difference $\beta_\text{diff}$ is between -50 and 50 msecs which we considered negligible.


## TODO Bayes Factor

Add info from Savage Dickey Bayes Factor slide


## Note on using more than a single IV

The Stan code presented in this walk through can be used to estimate the posterior for two and more conditions: We can calculate the difference between conditions from the posterior as well as main effects and interactions for more complex factorial designs as any factorial design can be reduced to a single variable.^[For example, say we have a 2 $\times$ 2 factorial design with factor 1 having two levels AB and CD and factor 2 having the corresponding levels AC and BD. These two factors render four conditions A, B, C, D. From posterior samples for each level A through D we can calculate main effect 1 as $\text{ME1}=(A+B) - (C+D)$, main effect 2 as $\text{ME2}=(A+C) - (B+D)$, and their interaction as $\text{Interaction}=(A-B) - (C-D)$ (or $\text{Interaction}=(A-C) - (B-D)$) and summary statistics as shown above.]





## By-participant estimates 

In some contexts we would like to obtain by-participant estimates of the probability of a participant to exhibit a disfluency (i.e. $\theta_s$). These estimates inform us about the prevalence of typing hesitations.

As before we can extract the posterior from the model `fit_molg`. This time we use the parameter that stored by-participant estimates `prob_s` (which is the inverse-logit of the `theta_s` which we didn't store in the model fit) corresponding to the population estimate `theta`. 

```{r}
posterior_ppts <- as.data.frame(fit_molg, pars = 'prob_s') %>% as_tibble()
names(posterior_ppts)[1:10]
```

This posterior tibble has the format

| "$<\text{parameter name}>\text{_s}[<\text{transition location id}>, <\text{participant id}>]$" 

where indices indicate the transition-location id and the participant identifier of the parameters. The following code creates a long format with columns `name` with the names above as levels and a `value` column with their respective values (both `name` and `value` are `pivot_longer` defaults). The `name` column is then separated into `name` (levels: `prob`), `location` (levels: `1`, `2`, `3`), and participant (an index for each participant) using "," to separate the three variables. 


```{r}
posterior_ppts_long <- posterior_ppts %>%
  pivot_longer(everything()) %>%
  # separate into location and ppt using "," as separator
  separate(name, into = c("location", "participant"), sep = ",") %>% 
  # extract number from new variables and recode location
  mutate(across(c(location, participant), parse_number),
         across(location, recode, 
                `1` = "before-sentence",
                `2` = "before-word",
                `3` = "within-word")) %>%
  # create different columns for location
  group_by(location) %>% 
  mutate(id = row_number()) %>%
  pivot_wider(names_from = location, values_from = value) %>% 
  select(-id) # drop id

posterior_ppts_long
```

We can then summarise by-participant estimates for each transition location with the most probable parameter value and the 95% PI as before.

```{r}
posterior_ppts_long_summary <- posterior_ppts_long %>%
  pivot_longer(-participant, names_to = "location") %>%
  group_by(location, participant) %>%
  summarise(across(value, list(mean = base::mean, 
                               lower = lower, 
                               upper = upper), .names = '{.fn}')) %>% 
  ungroup()

posterior_ppts_long_summary
```

For all 10 participants we see the estimate probability of typing hesitations by transition location. We observe in general that hesitations are more frequent before sentences and words than within words. However, participants vary to the extend they hesitate before words and sentences suggesting that some participant plan sentences more globally and others plan sentences word-by-word (incrementally).


```{r}
posterior_ppts_long_summary %>%
  ggplot(aes(x = participant, 
             y = mean, ymin = lower, ymax = upper,
            colour = location,
            shape = location)) +
  geom_pointrange(position = position_dodge(.75)) +
  labs(x = "Participant id", 
       y = "Parameter estimates with 95% PIs") +
  scale_colour_brewer("Transition\nlocation", palette = "Dark2") +
  scale_shape_manual("Transition\nlocation", values = c(21, 3, 8)) +
  scale_x_continuous(breaks = 1:10) +
  theme(legend.position = 'top')
```


Finally, we can calculate and visualise the by-participant differences. 

```{r}
posterior_ppts_long_diff_summary <- posterior_ppts_long %>% 
  mutate(diff_sent = `before-sentence` - `before-word`,
         diff_word = `before-word` - `within-word`) %>% 
  select(participant, starts_with('diff')) %>%
  pivot_longer(-participant, 
               names_to = 'difference', 
               values_to = 'diff') %>% 
  group_by(participant, difference) %>%
  summarise(across(diff, list(mean = base::mean, 
                               lower = lower, 
                               upper = upper), .names = '{.fn}')) %>% 
  ungroup()

posterior_ppts_long_diff_summary
```

This calculation and visualisation allows researchers to identify participants that do or do not differ in terms of hesitation frequency, in our example depending on transition locations.

```{r }
posterior_ppts_long_diff_summary %>%
  mutate(across(difference, recode, 
                diff_sent = "Comparison: before-sentence vs before-word",
                diff_word = "Comparison: before-word vs within-word")) %>% 
  ggplot(aes(x = factor(as.numeric(participant)), 
             y = mean, ymin = lower, ymax = upper)) +
  geom_hline(yintercept = 0, colour = "red", linetype = "dashed") +
  geom_pointrange(position = position_dodge(.75)) +
  facet_wrap(~difference) +
  coord_flip() +
  labs(x = "Participant id", 
       y = "Difference between transition locations") 
```


# References

<div id="refs"></div>

# Stan code

```{r echo = F}
cat(readLines("stan/molg.stan"), sep = "\n")
```

# Session info

```{r}
sessionInfo()
```
